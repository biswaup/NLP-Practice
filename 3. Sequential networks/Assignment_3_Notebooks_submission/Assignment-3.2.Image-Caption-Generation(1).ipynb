{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment-3.2.Image-Caption-Generation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6nfYHPZf3p0I","colab_type":"text"},"source":["## Assignment 3.2. Image Caption Generation"]},{"cell_type":"markdown","metadata":{"id":"WiPq8VLc3p0K","colab_type":"text"},"source":["## Task 2.1: Encoder Decoder Model"]},{"cell_type":"markdown","metadata":{"id":"IQua4M743p0L","colab_type":"text"},"source":["Build an image caption generator model, as described in Vinyals, Oriol, et al. \"Show and tell: A neural image caption generator.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. The model shall consist of:\n","\n","- Image encoder (image feature extractor)\n","- Caption generator (RNN-based)\n","\n"]},{"cell_type":"code","metadata":{"id":"kncr-iEXuSYh","colab_type":"code","outputId":"b9b0a2f2-eb56-4212-d3c8-1366b14458a6","executionInfo":{"status":"ok","timestamp":1559347693125,"user_tz":-120,"elapsed":1673,"user":{"displayName":"Upasana Biswas","photoUrl":"https://lh6.googleusercontent.com/-v_JP6nlPeLU/AAAAAAAAAAI/AAAAAAAABBM/FlzoIRKVWwA/s64/photo.jpg","userId":"15447145155754903867"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","from numpy import array\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import string\n","import os\n","from PIL import Image\n","import glob\n","from pickle import dump, load\n","from time import time\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import LSTM, GRU, Embedding, TimeDistributed, Dense, RepeatVector, Add, Lambda,\\\n","                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n","from keras.optimizers import Adam, RMSprop\n","from keras.layers.wrappers import Bidirectional\n","from keras.layers.merge import add\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.preprocessing import image\n","from keras.models import Model\n","from keras import Input, layers\n","from keras import optimizers\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","import keras.backend as K"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"R3PbOIHutLZO","colab_type":"code","colab":{}},"source":["abs_path = '/content/drive/My Drive/Recommender Systems/Assignment 3 - Sequential networks/'\n","files_path = abs_path + 'Assignment_3_Notebooks_submission/pickel_files/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzOTKzhJy1Ej","colab_type":"code","colab":{}},"source":["# train_features (images)\n","# train_descriptions (text) ==> We need to upload (train_descriptions, dev_descriptions, test_descriptions) as Pickel files!!!!!\n","# wordtoix as Pickel file!!!\n","# num_photos_per_batch computed later\n","# embedded_matrix as Pickel file!!!!"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Us3XyqFr9jy","colab_type":"text"},"source":["## Upload Data\n"]},{"cell_type":"code","metadata":{"id":"yjHdVnjksC4D","colab_type":"code","outputId":"1b262036-8dd3-46e7-944d-048092cb461e","executionInfo":{"status":"error","timestamp":1559348208569,"user_tz":-120,"elapsed":584,"user":{"displayName":"Upasana Biswas","photoUrl":"https://lh6.googleusercontent.com/-v_JP6nlPeLU/AAAAAAAAAAI/AAAAAAAABBM/FlzoIRKVWwA/s64/photo.jpg","userId":"15447145155754903867"}},"colab":{"base_uri":"https://localhost:8080/","height":366}},"source":["import _pickle as cPickle\n","\n","def read_pickle(data_path, file_name):\n","\n","    f = open(os.path.join(data_path, file_name), 'rb')\n","    read_file = cPickle.load(f)\n","    f.close()\n","\n","    return read_file\n","  \n","\n","def read_npy(data_path, file_name):\n","\n","    f = open(os.path.join(data_path, file_name), 'rb')\n","    read_file = np.load(f)\n","    f.close()\n","\n","    return read_file\n","\n","# Import Images Pickel Files\n","train_features = read_pickle(files_path, 'encoded_train_images.pkl')\n","dev_features = read_pickle(files_path, 'encoded_dev_images.pkl')\n","test_features = read_pickle(files_path, 'encoded_test_images.pkl')\n"," \n","# Import Caption Files (descriptions)\n","train_descriptions = read_pickle(files_path, 'train_descriptions.pkl')\n","dev_descriptions = read_pickle(files_path, 'dev_descriptions.pkl')\n","test_descriptions = read_pickle(files_path, 'test_descriptions.pkl')\n","\n","# Import Wordtoix dictionary\n","wordtoix = read_pickle(files_path, 'wordtoix.pkl')\n","\n","# Import Embedded_matrix\n","embedding_matrix = read_npy(files_path, 'embedding_matrix.npy')\n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-b3e2a4ba2e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Import Images Pickel Files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoded_train_images.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mdev_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoded_dev_images.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoded_test_images.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-b3e2a4ba2e52>\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(data_path, file_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mread_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Recommender Systems/Assignment 3 - Sequential networks/Assignment_3_Notebooks_submission/pickel_files/encoded_train_images.pkl'"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"13GNLSaWu7np"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"tOLFWVHIg7aF","colab_type":"code","colab":{}},"source":["K.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vGAAS9VukBp","colab_type":"code","colab":{}},"source":["# hardcoded variables\n","rnn_dim = 300; embedding_dim = 300\n","vocab_size = 1652\n","decoder_length = 34\n","max_length = 34\n","# embedding_matrix = np.zeros((vocab_size, embedding_dim))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flJyjEIg3p0M","colab_type":"text"},"source":["### Image Encoder"]},{"cell_type":"code","metadata":{"id":"Ekp5lJ-q3p0N","colab_type":"code","colab":{}},"source":["# image input\n","image_in = Input(shape=(2048,), name='image_inputs')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdP8kz8M3p0R","colab_type":"code","outputId":"f8ada33e-bec5-4546-ae9c-cdd5fbbe1108","executionInfo":{"status":"ok","timestamp":1559322403183,"user_tz":-120,"elapsed":497,"user":{"displayName":"Upasana Biswas","photoUrl":"https://lh6.googleusercontent.com/-v_JP6nlPeLU/AAAAAAAAAAI/AAAAAAAABBM/FlzoIRKVWwA/s64/photo.jpg","userId":"15447145155754903867"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["# image encoder\n","fe1 = Dropout(0.5, name='dropout_img_feats')(image_in)\n","image_dense = Dense(rnn_dim, activation='relu', name = 'dense_img_feats')\n","fe2 = image_dense(fe1) # reduce the dimension with FC projection"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mz_AduEO3p0V","colab_type":"text"},"source":["### Caption Generator"]},{"cell_type":"code","metadata":{"id":"6gDA5leg7n1V","colab_type":"code","colab":{}},"source":["# caption input\n","cap_in = Input(shape=(None,),name='caption_inputs') \n","\n","# caption embedding representation (word-based embedding)\n","embed_cap = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], trainable = False)\n","embed_cap_out = embed_cap(cap_in)\n","drop_cap = Dropout(0.5)\n","se2 = drop_cap(embed_cap_out)\n","\n","\n","## encoder\n","lstm_encoder = LSTM(rnn_dim,  name='lstm_encoder')\n","# lstm_encoder = GRU(rnn_dim, return_state=True, name='lstm_encoder')\n","se3 = lstm_encoder(se2)\n","\n","\n","###### should we include this???\n","# decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n","# decoder_embedding = Embedding(NL_VOCAB_SIZE, NL_EMBEDDING_DIM, trainable = True, \n","#               weights=[embedding_nl], name='embedding_decoder')\n","# embedding_output = decoder_embedding(decoder_inputs)\n","##########\n","\n","# state input for each decoder time step\n","s0 = Input(shape=(rnn_dim,), name='s0') # with a dimension of (, rnn_dim)\n","s = [s0]\n","\n","\n","# layers initialization\n","# LSTM/GRU decoder as caption generator\n","decoder = GRU(rnn_dim, return_state=True)\n","decoder_dense = Dense(rnn_dim, activation='relu')\n","\n","\n","# Prediction layer with softmax activation\n","pred_layer = Dense(vocab_size, activation='softmax')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QEYM9bN4MKl","colab_type":"code","colab":{}},"source":["# print(pred_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YO2K4MQoM6hq","colab_type":"code","colab":{}},"source":["probs = []\n","\n","# process the training per time step (following the max length of captions)\n","for t in range(decoder_length):\n","    \n","    ### YOUR CODE HERE\n","    x_dec = Lambda(lambda x: x[:,t,:], name='dec_embedding-%s'%t)(se2)\n","    x_dec = Reshape((1, embedding_dim))(x_dec)\n","    \n","    ### ... HOW DO YOU REPRESENT JOINT-REPRESENTATION OF IMAGE-CAPTION AS DECODER INPUT? \n","    enc_out_reshape = Reshape((1, embedding_dim))(fe2)\n","    context_concat = add([x_dec, enc_out_reshape])\n","    #context_concat = concatenate([x_dec, enc_out_reshape],axis=-1)\n","\n","    \n","    ### ... HOW DO YOU INITIALIZE THE RNN-BASED DECODER STATE IN TIME STEP=0? \n","    \n","    if t == 0:\n","      s = se3\n","        \n","    ### ... WHAT IS THE INPUT OF THE DECODER? \n","    \n","    \n","    s, _ = decoder(context_concat, initial_state = s)\n","    \n","    # softmax probability output\n","    prob = pred_layer(s)\n","    \n","    probs.append(prob)\n","    s = [s]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_w7hiF-3p0V","colab_type":"code","colab":{}},"source":["# # caption input\n","# cap_in = ### YOUR CODE HERE\n","\n","# # caption embedding representation (word-based embedding)\n","# embed_cap = ### YOUR CODE HERE\n","\n","# # state input for each decoder time step\n","# s0 = Input(shape=(rnn_dim,), name='s0') # with a dimension of (, rnn_dim)\n","# s = [s0]\n","\n","# # LSTM/GRU decoder as caption generator\n","# decoder = ### YOUR CODE HERE\n","\n","# # Prediction layer with softmax activation\n","# pred_layer = Dense(vocab_size, activation='softmax')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rv2U_Rd93p0Y","colab_type":"code","colab":{}},"source":["# probs = []\n","\n","# # process the training per time step (following the max length of captions)\n","# for t in range(decoder_length):\n","    \n","#     ### YOUR CODE HERE\n","    \n","#     ### ... HOW DO YOU REPRESENT JOINT-REPRESENTATION OF IMAGE-CAPTION AS DECODER INPUT? \n","    \n","#     ### ... HOW DO YOU INITIALIZE THE RNN-BASED DECODER STATE IN TIME STEP=0? \n","    \n","#     if t == 0:\n","        \n","#     ### ... WHAT IS THE INPUT OF THE DECODER? \n","    \n","    \n","#     s, _ = ### YOUR CODE HERE\n","    \n","#     # softmax probability output\n","#     prob = pred_layer(s)\n","    \n","#     probs.append(prob)\n","#     s = [s]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMepLh0G3p0b","colab_type":"text"},"source":["### The model shall be constructed based on the following inputs"]},{"cell_type":"code","metadata":{"id":"DXuiPEGC3p0c","colab_type":"code","colab":{}},"source":["# Construct the model\n","model = Model(inputs=[image_in, cap_in, s0], outputs=probs)\n","# Compile & run training\n","adam = optimizers.Adam(lr=0.001)\n","model.compile(optimizer=adam, loss='categorical_crossentropy')\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_FbvSJY3p0f","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"oGGZAWlh3p0g","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdJZc_Srw7Ly","colab_type":"code","colab":{}},"source":["# train_descriptions, train_features, wordtoidx, max_length, number_pics_per_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddUhRfHLws7J","colab_type":"code","colab":{}},"source":["# # data generator, intended to be used in a call to model.fit_generator()\n","# def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n","  \n","#     X1, X2, y = list(), list(), list()\n","#     n=0\n","#     # loop for ever over images\n","#     while 1:\n","#         for key, desc_list in descriptions.items():\n","#             n+=1\n","#             # retrieve the photo feature\n","#             photo = photos[key+'.jpg']\n","#             for desc in desc_list:\n","#                 # encode the sequence\n","#                 seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n","#                 # split one sequence into multiple X, y pairs\n","#                 for i in range(1, len(seq)):\n","#                     # split into input and output pair\n","#                     in_seq, out_seq = seq[:i], seq[i]\n","#                     # pad input sequence\n","#                     in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","#                     # encode output sequence\n","#                     out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","#                     # store\n","#                     X1.append(photo)\n","#                     X2.append(in_seq)\n","#                     y.append(out_seq)\n","#             # yield the batch data\n","#             if n==num_photos_per_batch:\n","#                 yield [[array(X1), array(X2)], array(y)]\n","#                 X1, X2, y = list(), list(), list()\n","#                 n=0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CIlZklawr4G","colab_type":"code","colab":{}},"source":["# epochs = 10\n","# number_pics_per_batch = 3\n","# train_steps = len(train_descriptions)//number_pics_per_batch\n","# dev_steps = len(dev_descriptions)//number_pics_per_batch\n","# Callback!!!\n","\n","\n","# for i in range(epochs):\n","#     train_generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_batch)\n","#     dev_generator = data_generator(dev_descriptions, dev_features, wordtoix, max_length, number_pics_per_batch)\n","#     model.fit_generator(generator, epochs=1, steps_per_epoch=train_steps, verbose=1, callbacks=None, validation_data=dev_generator, validation_steps=dev_steps)\n","#     model.save('./model_' + str(i) + '.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVjFFU0B3p0j","colab_type":"text"},"source":["## Task 2.2: Decoder Model"]},{"cell_type":"markdown","metadata":{"id":"m5Vw4laH3p0k","colab_type":"text"},"source":["Based on the completed encoder-decoder, build a decoder model for generating captions using two approaches:\n","- Greedy search\n","- Beam search"]},{"cell_type":"code","metadata":{"id":"Q3mn8OGk3p0l","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDOU5Xf73p0o","colab_type":"text"},"source":["### Greedy search"]},{"cell_type":"code","metadata":{"id":"jps-ujmm3p0p","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BS3yM7xA3p0s","colab_type":"text"},"source":["### Beam search"]},{"cell_type":"code","metadata":{"id":"8-kj-2oc3p0s","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]}]}