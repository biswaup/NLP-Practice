{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment-3.1.1.Document-Level-Sentiment.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-CB404F4Zlcu","colab_type":"text"},"source":["## Assignment 3.1. Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"b07EkGYLZlc1","colab_type":"text"},"source":["## Task 1.1: Document-level Sentiment Classification"]},{"cell_type":"markdown","metadata":{"id":"vYUQ1MD2Zlc5","colab_type":"text"},"source":["Build a Bidirectional Recurrent Neural Network (RNN) model for multi-class sentiment classification. Compare the performance with a Unidirectional RNN model. Your model (each) shall\n","include:\n","\n","- RNN network that learns sentence representation from input sequences.\n","- Fully connected network that predicts sentiment label, given the learnt state representation.\n","\n","\n","Train the model by using data iterator and batch generator. Evaluate the trained model on\n","the provided test set."]},{"cell_type":"code","metadata":{"id":"sXwpbv-ab0m9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"5a988b03-b611-4e48-b8d7-f3355e7ace7e","executionInfo":{"status":"ok","timestamp":1559567106195,"user_tz":-120,"elapsed":32053,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XQjAYPr1cSVX","colab_type":"code","colab":{}},"source":["absolute_path = '/content/drive/My Drive/TUE -EIT (me)/Recommender Systems/Assignment 3 - Sequential networks/Document Level/'\n","data_path = absolute_path + 'data/doc_level-sentiment/doc_level'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqr2TjHTcGnV","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import codecs\n","import operator\n","import numpy as np\n","import re\n","from time import time\n","import matplotlib.pyplot as plt\n","import _pickle as cPickle\n","import operator\n","%matplotlib notebook\n","plt.style.use('ggplot')\n","\n","from keras.preprocessing import sequence\n","from keras.utils.np_utils import to_categorical\n","import keras.backend as K\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3rdJW5pkXUl","colab_type":"code","colab":{}},"source":["# parameters\n","maxlen = 300\n","batch_size = 32"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4WRpGmElcdDy","colab_type":"code","colab":{}},"source":["# Reading the pickle files\n","\n","def read_pickle(data_path, file_name):\n","\n","    f = open(os.path.join(data_path, file_name), 'rb')\n","    read_file = cPickle.load(f)\n","    f.close()\n","\n","    return read_file\n","\n","def save_pickle(data_path, file_name, data):\n","\n","    f = open(os.path.join(data_path, file_name), 'wb')\n","    cPickle.dump(data, f)\n","    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n","    f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahxQS07Rcjkt","colab_type":"code","colab":{}},"source":["words_idx = read_pickle(data_path, 'words_idx.pkl')\n","idx_words = read_pickle(data_path, 'idx_words.pkl')\n","data = read_pickle(data_path, 'data.pkl')\n","label = read_pickle(data_path, 'label.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmtmfXuBc08o","colab_type":"code","colab":{}},"source":["# Creating train and test sets\n","\n","rand_idx = np.arange(len(data))\n","np.random.shuffle(rand_idx)\n","\n","data = data[rand_idx]\n","label = to_categorical(label)[rand_idx]\n","\n","data_size = len(data)\n","\n","test_x = data[0:1000]\n","test_y = label[0:1000]\n","\n","dev_x = data[1000:5000]\n","dev_y = label[1000:5000]\n","\n","train_x = data[5000:int(data_size)]\n","train_y = label[5000:int(data_size)]\n","\n","words_idx = [x for (x, _) in sorted(words_idx.items(), key=operator.itemgetter(1))]\n","\n","train_x_ = sequence.pad_sequences(train_x, maxlen)\n","dev_x_ = sequence.pad_sequences(dev_x, maxlen)\n","test_x_ = sequence.pad_sequences(test_x, maxlen)\n","\n","train_x_ = np.array(train_x_)\n","train_y = np.array(train_y)\n","dev_x_ = np.array(dev_x_)\n","dev_y = np.array(dev_y)\n","test_x_ = np.array(test_x_)\n","test_y = np.array(test_y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_o2Dg4jMlDAE","colab_type":"code","colab":{}},"source":["# Data iterator\n","\n","class Dataiterator():\n","    '''\n","      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n","      2) Access to the entire dataset using all()\n","    '''\n","    \n","    def __init__(self, X, y, seq_length=32, decoder_dim=300, batch_size=32):      \n","        self.X = X \n","        self.y = y \n","        self.num_data = len(X) # total number of examples\n","        self.batch_size = batch_size # batch size\n","        self.reset() # initial: shuffling examples and set index to 0\n","    \n","    def __iter__(self): # iterates data\n","        return self\n","\n","\n","    def reset(self): # initials\n","        self.idx = 0\n","        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n","        \n","    def __next__(self): # return model inputs - outputs per batch\n","        X_ids = [] # hold ids per batch \n","        while len(X_ids) < self.batch_size:\n","            X_id = self.order[self.idx] # copy random id from initial shuffling\n","            X_ids.append(X_id)\n","            self.idx += 1 # \n","            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n","                self.reset()\n","                raise StopIteration()\n","        batch_X = self.X[np.array(X_ids)] # X values (encoder input) per batch\n","        batch_y = self.y[np.array(X_ids)] # y_in values (decoder input) per batch\n","        return batch_X, batch_y\n","\n","          \n","    def all(self): # return all data examples\n","        return self.X, self.y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"13GNLSaWu7np"},"source":["## Unidirectional RNN Model for document level sentiment classification"]},{"cell_type":"code","metadata":{"id":"v3I5jfM4DBVL","colab_type":"code","colab":{}},"source":["# model parameters\n","dropout= 0.5\n","recurrent_dropout = 0.3\n","early_stop_patience = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OBf-fN05ZldE","colab_type":"code","colab":{}},"source":["K.clear_session()\n","\n","from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Input, Bidirectional\n","from keras.models import Model\n","import keras.optimizers as opt\n","\n","# input layer\n","sentence_input = Input(shape=(None,), dtype='int32', name='sentence_input')\n","\n","# layer to train the emedding\n","vocab_size = len(words_idx)\n","word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')\n","emb_output = word_emb(sentence_input)\n","\n","# RNN-based layer\n","lstm_layer = LSTM(300, return_sequences=False, dropout=dropout, \\\n","              recurrent_dropout=recurrent_dropout, name='lstm')(emb_output)\n","\n","# prediction layer\n","densed = Dense(3, name='dense')(lstm_layer)\n","probs = Activation('softmax')(densed)\n","\n","# model\n","model = Model(inputs=[sentence_input], outputs=probs)\n","optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnW2M53ICOXU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":312},"outputId":"ed063308-2029-4de9-94fb-d71439cd976d","executionInfo":{"status":"ok","timestamp":1559577031717,"user_tz":-120,"elapsed":441,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}}},"source":["model.summary()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","sentence_input (InputLayer)  (None, None)              0         \n","_________________________________________________________________\n","word_emb (Embedding)         (None, None, 300)         3000900   \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 300)               721200    \n","_________________________________________________________________\n","dense (Dense)                (None, 3)                 903       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 3)                 0         \n","=================================================================\n","Total params: 3,723,003\n","Trainable params: 3,723,003\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LUPoRneRZleH","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"8bbrPU9VCm2q","colab_type":"code","colab":{}},"source":["def train_generator(model, batch_train_iter, batch_val_iter):\n","    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=early_stop_patience),\n","                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n","                                     monitor='val_loss', save_best_only=False, \\\n","                                     save_weights_only=True)\n","                     ]\n","    \n","    def train_gen():\n","        while True:\n","            train_batches = [[X, y] for X, y in batch_train_iter]\n","            for train_batch in train_batches:\n","                yield train_batch\n","                \n","    def val_gen():\n","        while True:\n","            val_batches = [[X, y] for X, y in batch_val_iter]\n","            for val_batch in val_batches:\n","                yield val_batch\n","                \n","    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n","                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n","                                  epochs = 20, callbacks = earlystop_callbacks)\n","      \n","    return history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RP6YyuJJZleK","colab_type":"code","colab":{}},"source":["# train batch\n","train_steps_epoch = len(train_x_)/batch_size\n","batch_train_iter = Dataiterator(train_x_, train_y, batch_size)\n","\n","# validation batch\n","val_steps_epoch = len(dev_x_)/batch_size\n","batch_val_iter = Dataiterator(dev_x_, dev_y, batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iqOr1_wVCtoC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":346},"outputId":"3d95dafc-20c9-4a8c-adc7-1868ae3e4536","executionInfo":{"status":"error","timestamp":1559577673573,"user_tz":-120,"elapsed":393225,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}}},"source":["history_unidirectional = train_generator(model, batch_train_iter, batch_val_iter)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","144/781 [====>.........................] - ETA: 28:39 - loss: 1.0054 - categorical_accuracy: 0.5156"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-8f38170968e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_unidirectional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_train_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_val_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-7b4fc5a5f2ea>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, batch_train_iter, batch_val_iter)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps_epoch\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearlystop_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"dMN7ytRFDS7f","colab_type":"code","colab":{}},"source":["# printing the loss\n","plt.plot(history_unidirectional.history['loss'])\n","plt.plot(history_unidirectional.history['val_loss'])\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Categorical CrossEntropy Loss')\n","plt.title('Loss Over Time')\n","plt.legend(['Train','Validation'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"306VCTuADYmz","colab_type":"code","colab":{}},"source":["# printing the accuracy\n","plt.plot(history_unidirectional.history['categorical_accuracy'])\n","plt.plot(history_unidirectional.history['val_categorical_accuracy'])\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Categorical Accuracy')\n","plt.title('Categorical accuracy Over Time')\n","plt.legend(['Train','Validation'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9akv0FhZleW","colab_type":"text"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"id":"WWO-9CDAZlfO","colab_type":"code","colab":{}},"source":["# evaluate on test_set\n","test_loss, test_acc = model.evaluate(test_x_, test_y, verbose = 2, batch_size = 32)\n","print(\"Test loss: %.2f\" % (test_loss))\n","print(\"Test accuracy: %.2f\" % (test_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JuE1ZyNZlfu","colab_type":"text"},"source":["## Bidirectional RNN Model for document level sentiment classification"]},{"cell_type":"code","metadata":{"id":"5-4pLaEqZlfw","colab_type":"code","colab":{}},"source":["bidirectional_units = 150\n","\n","sentence_input = Input(shape=(None,), dtype='int32', name='sentence_input')\n","vocab_size = len(words_idx)\n","word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')\n","emb_output = word_emb(sentence_input)\n","\n","dropout= 0.5\n","recurrent_dropout = 0.1 \n","lstm_layer = Bidirectional(LSTM(bidirectional_units, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout, name='bilstm'))(emb_output)\n","densed = Dense(3, name='dense')(lstm_layer)\n","probs = Activation('softmax')(densed)\n","biModel = Model(inputs=[sentence_input], outputs=probs)\n","optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n","biModel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","biModel.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iv7C8RZyZlf2","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"2lW9tXhbZlf3","colab_type":"code","colab":{}},"source":["history_bidirectional = train_generator(biModel, batch_train_iter, batch_val_iter)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFwRGs70Dz11","colab_type":"code","colab":{}},"source":["# Printing the Loss\n","plt.plot(history_bidirectional.history['loss'])\n","plt.plot(history_bidirectional.history['val_loss'])\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Categorical CrossEntropy Loss')\n","plt.title('Loss Over Time')\n","plt.legend(['Train','Validation'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipbIrcrUD6dY","colab_type":"code","colab":{}},"source":["## Printing the Accuracy\n","plt.plot(history_bidirectional.history['categorical_accuracy'])\n","plt.plot(history_bidirectional.history['val_categorical_accuracy'])\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Categorical Accuracy')\n","plt.title('Categorical accuracy Over Time')\n","plt.legend(['Train','Validation'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MaQ9mlYHZlf9","colab_type":"text"},"source":["### Evaluate"]},{"cell_type":"code","metadata":{"id":"R2W_tGiWZlf_","colab_type":"code","colab":{}},"source":["# Evaluation on test_set\n","test_loss, test_acc = biModel.evaluate(test_x_, test_y, verbose = 2, batch_size = 32)\n","print(\"Test loss: %.2f\" % (test_loss))\n","print(\"Test accuracy: %.2f\" % (test_acc))"],"execution_count":0,"outputs":[]}]}