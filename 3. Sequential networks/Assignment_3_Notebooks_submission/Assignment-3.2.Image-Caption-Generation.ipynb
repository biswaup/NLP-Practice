{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment-3.2.Image-Caption-Generation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jt0RwYrLPAXE","colab_type":"text"},"source":["## Assignment 3.2. Image Caption Generation"]},{"cell_type":"markdown","metadata":{"id":"GTRkc9o2PAXI","colab_type":"text"},"source":["## Task 2.1: Encoder Decoder Model"]},{"cell_type":"markdown","metadata":{"id":"qmUCHDpHPAXN","colab_type":"text"},"source":["Build an image caption generator model, as described in Vinyals, Oriol, et al. \"Show and tell: A neural image caption generator.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. The model shall consist of:\n","\n","- Image encoder (image feature extractor)\n","- Caption generator (RNN-based)\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"13GNLSaWu7np"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"28DCb6h9PAXX","colab_type":"text"},"source":["### Image Encoder"]},{"cell_type":"code","metadata":{"id":"21vsLCzfPAXa","colab_type":"code","colab":{}},"source":["# image input\n","image_in = Input(shape=(2048,), name='image_inputs')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3daQMY1pPAXq","colab_type":"code","colab":{}},"source":["# image encoder\n","fe1 = Dropout(0.5, name='dropout_img_feats')(image_in)\n","image_dense = Dense(rnn_dim, activation='relu', name = 'dense_img_feats')\n","fe2 = image_dense(fe1) # reduce the dimension with FC projection"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHKlXk-DPAX4","colab_type":"text"},"source":["### Caption Generator"]},{"cell_type":"code","metadata":{"id":"VqlgNm-1PAX9","colab_type":"code","colab":{}},"source":["# caption input\n","cap_in = ### YOUR CODE HERE\n","\n","# caption embedding representation (word-based embedding)\n","embed_cap = ### YOUR CODE HERE\n","\n","# state input for each decoder time step\n","s0 = Input(shape=(rnn_dim,), name='s0') # with a dimension of (, rnn_dim)\n","s = [s0]\n","\n","# LSTM/GRU decoder as caption generator\n","decoder = ### YOUR CODE HERE\n","\n","# Prediction layer with softmax activation\n","pred_layer = Dense(vocab_size, activation='softmax')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQysFGnpPAYK","colab_type":"code","colab":{}},"source":["probs = []\n","\n","# process the training per time step (following the max length of captions)\n","for t in range(decoder_length):\n","    \n","    ### YOUR CODE HERE\n","    \n","    ### ... HOW DO YOU REPRESENT JOINT-REPRESENTATION OF IMAGE-CAPTION AS DECODER INPUT? \n","    \n","    ### ... HOW DO YOU INITIALIZE THE RNN-BASED DECODER STATE IN TIME STEP=0? \n","    \n","    if t == 0:\n","        \n","    ### ... WHAT IS THE INPUT OF THE DECODER? \n","    \n","    \n","    s, _ = ### YOUR CODE HERE\n","    \n","    # softmax probability output\n","    prob = pred_layer(s)\n","    \n","    probs.append(prob)\n","    s = [s]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23kt7UzgPAYe","colab_type":"text"},"source":["### The model shall be constructed based on the following inputs"]},{"cell_type":"code","metadata":{"id":"3G4pFbtUPAYi","colab_type":"code","colab":{}},"source":["# Construct the model\n","model = Model(inputs=[image_in, cap_in, s0], outputs=probs)\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1qijSSfrPAYu","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"8jVOMRKbPAYx","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h18H9JUHPAZA","colab_type":"text"},"source":["## Task 2.2: Decoder Model"]},{"cell_type":"markdown","metadata":{"id":"xsJX3hlTPAZG","colab_type":"text"},"source":["Based on the completed encoder-decoder, build a decoder model for generating captions using two approaches:\n","- Greedy search\n","- Beam search"]},{"cell_type":"code","metadata":{"id":"gR2zqPqdPAZK","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ItAzF_iFPAZY","colab_type":"text"},"source":["### Greedy search"]},{"cell_type":"code","metadata":{"id":"4fgQdrRTPAZd","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VMv8pzvPAZq","colab_type":"text"},"source":["### Beam search"]},{"cell_type":"code","metadata":{"id":"AAmJU56MPAZu","colab_type":"code","colab":{}},"source":["### YOUR CODE HERE"],"execution_count":0,"outputs":[]}]}