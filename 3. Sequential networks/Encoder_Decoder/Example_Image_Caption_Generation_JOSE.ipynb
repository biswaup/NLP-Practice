{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Example_Image_Caption_Generation_JOSE.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"DqlmK5wzQ63n","outputId":"422cccb3-0462-4ba9-93b6-52f8a78dd379","executionInfo":{"status":"ok","timestamp":1559303801065,"user_tz":-120,"elapsed":49558,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u-PuAcJ6uWWi","colab_type":"code","colab":{}},"source":["abs_path = 'D:/Upasana/Semester 3/Recommender Systems/Assignment 3/5.2. Sequence-to-Sequence/'\n","glov_path = 'D:/Upasana/Semester 3/Recommender Systems/Assignment 3'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZovR9HfSRHX_","outputId":"489411d3-c115-4c88-bda8-6b6baf95b2a6","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#cd drive/My Drive/Recsys-2019/image_caption_gen"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Recsys-2019/image_caption_gen\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"twIMBYIDU2Fg","colab":{}},"source":["import numpy as np\n","from numpy import array\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import string\n","import os\n","from PIL import Image\n","import glob\n","from pickle import dump, load\n","from time import time\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import LSTM, GRU, Embedding, TimeDistributed, Dense, RepeatVector, Add, Lambda,\\\n","                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n","from keras.optimizers import Adam, RMSprop\n","from keras.layers.wrappers import Bidirectional\n","from keras.layers.merge import add\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.preprocessing import image\n","from keras.models import Model\n","from keras import Input, layers\n","from keras import optimizers\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zkZZxrAQ1loE","outputId":"7919ec78-2dd1-405a-8db9-e142cc9dd666","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","filename = abs_path + \"flickerText/Flickr8k.token.txt\"\n","# load descriptions\n","doc = load_doc(filename)\n","print(doc[:1000])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n","1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n","1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n","1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n","1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n","1001773457_577c3a7d70.jpg#0\tA black dog and a spotted dog are fighting\n","1001773457_577c3a7d70.jpg#1\tA black dog and a tri-colored dog playing with each other on the road .\n","1001773457_577c3a7d70.jpg#2\tA black dog and a white dog with brown spots are staring at each other in the street .\n","1001773457_577c3a7d70.jpg#3\tTwo dogs of different breeds looking at each other on the road .\n","1001773457_577c3a7d70.jpg#4\tTwo dogs on pavement moving toward each other .\n","1002674143_1b742ab4b8.jpg#0\tA little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\n","1002674143_1b\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uKxPOBOg1yxQ","outputId":"61e9ee38-626f-41d6-858e-e9854b8336a0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def load_descriptions(doc):\n","  \n","    mapping = dict()\n","    # process lines\n","    for line in doc.split('\\n'):\n","        # split line by white space\n","        tokens = line.split()\n","        if len(line) < 2:\n","            continue\n","        # take the first token as the image id, the rest as the description\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        # extract filename from image id\n","        image_id = image_id.split('.')[0]\n","        # convert description tokens back to string\n","        image_desc = ' '.join(image_desc)\n","        # create the list if needed\n","        if image_id not in mapping:\n","            mapping[image_id] = list()\n","        # store description\n","        mapping[image_id].append(image_desc)\n","    \n","    return mapping\n","\n","# parse descriptions\n","# create a dictionary named “descriptions” which contains the \n","# name of the image (without the .jpg) as keys and \n","# a list of the 5 captions for the corresponding image as values\n","descriptions = load_descriptions(doc)\n","print('Loaded: %d ' % len(descriptions))\n","print(descriptions['1000268201_693b08cb0e'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded: 8092 \n","['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zGooZwOf33lL","outputId":"607e3f99-302b-4d53-8afc-5a52e477d0dd","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["list(descriptions.keys())[:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1000268201_693b08cb0e',\n"," '1001773457_577c3a7d70',\n"," '1002674143_1b742ab4b8',\n"," '1003163366_44323f5815',\n"," '1007129816_e794419615']"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PEQ0YpfO39UQ","outputId":"2f902690-17d3-439c-cb90-e96c0ecf21d6","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["descriptions['990890291_afc72be141']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A man does a wheelie on his bicycle on the sidewalk .',\n"," 'A man is doing a wheelie on a mountain bike .',\n"," 'A man on a bicycle is on only the back wheel .',\n"," 'Asian man in orange hat is popping a wheelie on his bike .',\n"," 'Man on a bicycle riding on only one wheel .']"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n_95gJ4O4Jfp","colab":{}},"source":["def clean_descriptions(descriptions):\n","    # prepare translation table for removing punctuation\n","    table = str.maketrans('', '', string.punctuation)\n","    for key, desc_list in descriptions.items():\n","        for i in range(len(desc_list)):\n","            desc = desc_list[i]\n","            # tokenize\n","            desc = desc.split()\n","            # convert to lower case\n","            desc = [word.lower() for word in desc]\n","            # remove punctuation from each token\n","            desc = [w.translate(table) for w in desc]\n","            # remove hanging 's' and 'a'\n","            desc = [word for word in desc if len(word)>1]\n","            # remove tokens with numbers in them\n","            desc = [word for word in desc if word.isalpha()]\n","            # store as string\n","            desc_list[i] =  ' '.join(desc)\n","\n","# clean descriptions\n","clean_descriptions(descriptions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dTs3Haax4KeS","outputId":"821c00ac-c97d-41b0-a2d4-7ba938fc17e8","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["descriptions['990890291_afc72be141']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['man does wheelie on his bicycle on the sidewalk',\n"," 'man is doing wheelie on mountain bike',\n"," 'man on bicycle is on only the back wheel',\n"," 'asian man in orange hat is popping wheelie on his bike',\n"," 'man on bicycle riding on only one wheel']"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4q1-PnI-4zsV","outputId":"45f48e89-f3ce-4a3a-bbe9-a0e01ab729c3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# convert the loaded descriptions into a vocabulary of \n","# unique words present across all the 8000*5 (i.e. 40000) image captions (corpus) in the data set\n","def to_vocabulary(descriptions):\n","# build a list of all description strings\n","    all_desc = set()\n","    for key in descriptions.keys():\n","        [all_desc.update(d.split()) for d in descriptions[key]]\n","    return all_desc\n","\n","# summarize vocabulary\n","vocabulary = to_vocabulary(descriptions)\n","print('Original Vocabulary Size: %d' % len(vocabulary))\n","print(list(vocabulary)[:10])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Original Vocabulary Size: 8763\n","['seater', 'literature', 'window', 'tentlike', 'decide', 'headline', 'model', 'scared', 'ribbons', 'emerald']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZHQ70Svp407n","colab":{}},"source":["# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","\tlines = list()\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor desc in desc_list:\n","\t\t\tlines.append(key + ' ' + desc)\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n","\n","save_descriptions(descriptions, abs_path + 'flickerText/descriptions.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Sg37brin5AzS","outputId":"8d9a434b-baba-446b-9096-a9f938c565d9","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# load a pre-defined list of photo identifiers\n","# read the image names from the train set, test set and dev set\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)\n","\n","# load training dataset (6K)\n","filename = abs_path + 'flickerText/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l8oDX3v2N0MH","outputId":"0f9855f7-f3c1-4918-d6be-748c001e3ef0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# load validation dataset (1K)\n","dev_fn = abs_path + 'flickerText/Flickr_8k.devImages.txt'\n","dev = load_set(dev_fn)\n","print('Validation Dataset: %d' % len(dev))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Validation Dataset: 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JJ1GfeFiOIy9","outputId":"f4ebe3a6-ecb7-407d-c122-900e1b00d829","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# load test dataset (1K)\n","test_fn = abs_path + 'flickerText/Flickr_8k.testImages.txt'\n","test = load_set(test_fn)\n","print('Test Dataset: %d' % len(test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Dataset: 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jUb-tiZL551e","outputId":"23cd4c41-494e-42ce-aa71-6a35337dcd8e","colab":{}},"source":["# Below path contains all the images\n","images = abs_path + 'Flicker8k_Dataset/'\n","# Create a list of all image names in the directory\n","img = glob.glob(images + '*.jpg')\n","print(len(img))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["8091\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KHSL1A6K6CGl","colab":{}},"source":["# Below file conatains the names of images to be used in train data\n","train_images_file = abs_path + 'flickerText/Flickr_8k.trainImages.txt'\n","\n","# Read the train image names in a set\n","train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n","\n","# Create a list of all the training images with their full path names\n","train_img = []\n","\n","for i in img: # img is list of full path names of all images\n","    if i[len(images):] in train_images: # Check if the image belongs to training set\n","        train_img.append(i) # Add it to the list of train images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MpDflomeOfMd","colab":{}},"source":["# Below file conatains the names of images to be used in validation data\n","dev_images_file = abs_path +  'flickerText/Flickr_8k.devImages.txt'\n","# Read the validation image names in a set# Read the test image names in a set\n","dev_images = set(open(dev_images_file, 'r').read().strip().split('\\n'))\n","\n","# Create a list of all the test images with their full path names\n","dev_img = []\n","\n","for i in img: # img is list of full path names of all images\n","    if i[len(images):] in dev_images: # Check if the image belongs to test set\n","        dev_img.append(i) # Add it to the list of test images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3rQ-MmL36H8W","colab":{}},"source":["# Below file conatains the names of images to be used in test data\n","test_images_file = abs_path + 'flickerText/Flickr_8k.testImages.txt'\n","# Read the validation image names in a set# Read the test image names in a set\n","test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n","\n","# Create a list of all the test images with their full path names\n","test_img = []\n","\n","for i in img: # img is list of full path names of all images\n","    if i[len(images):] in test_images: # Check if the image belongs to test set\n","        test_img.append(i) # Add it to the list of test images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gdox6a6K6LUn","outputId":"1006a047-3567-43db-9bc7-6b49a849ee6b","executionInfo":{"status":"error","timestamp":1559321120683,"user_tz":-120,"elapsed":722,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["# Now, we load the descriptions of these images from “descriptions.txt” in the Python dictionary “train_descriptions”.\n","# However, when we load them, we will add two tokens in every caption:\n","# ‘startseq’ -> This is a start sequence token which will be added at the start of every caption.\n","# ‘endseq’ -> This is an end sequence token which will be added at the end of every caption.\n","\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","# descriptions\n","train_descriptions = load_clean_descriptions(abs_path + 'flickerText/descriptions.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# print(train_descriptions)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9e7db8d1f874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean_descriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'flickerText/descriptions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Descriptions: train=%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# print(train_descriptions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'abs_path' is not defined"]}]},{"cell_type":"code","metadata":{"id":"c2As6TQ_x51k","colab_type":"code","outputId":"02ffcf92-260c-46a2-97eb-1123c82bc6b2","executionInfo":{"status":"error","timestamp":1559321111086,"user_tz":-120,"elapsed":753,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["print(type(train_descriptions))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-506d0f156df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_descriptions' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XnZ2RM4MO6E-","outputId":"6f656a72-62bf-469a-a727-ddfc9658e649","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# descriptions\n","dev_descriptions = load_clean_descriptions(abs_path + 'flickerText/descriptions.txt', dev)\n","print('Descriptions: validation=%d' % len(dev_descriptions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Descriptions: validation=1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qU83cVIxPK68","outputId":"f7cb810d-c637-493a-ed20-ba794395b551","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# descriptions\n","test_descriptions = load_clean_descriptions(abs_path + 'flickerText/descriptions.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Descriptions: test=1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3iMVcry06Xw0"},"source":["### Transfer learning from inception model who had been trained on ImageNet\n","Images are nothing but input (X) to our model. As you may already know that any input to a model must be given in the form of a vector.\n","We need to convert every image into a fixed sized vector which can then be fed as input to the neural network. For this purpose, we opt for transfer learning by using the InceptionV3 model (Convolutional Neural Network) created by Google Research.\n","This model was trained on Imagenet dataset to perform image classification on 1000 different classes of images. However, our purpose here is not to classify the image but just get fixed-length informative vector for each image. This process is called automatic feature engineering.\n","Hence, we just remove the last softmax layer from the model and extract a 2048 length vector (bottleneck features) for every image"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b3Un9mVM6R7D","colab":{}},"source":["def preprocess(image_path):\n","    # Convert all the images to size 299x299 as expected by the inception v3 model\n","    img = image.load_img(image_path, target_size=(299, 299))\n","    # Convert PIL image to numpy array of 3-dimensions\n","    x = image.img_to_array(img)\n","    # Add one more dimension\n","    x = np.expand_dims(x, axis=0)\n","    # preprocess the images using preprocess_input() from inception module\n","    x = preprocess_input(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x5YI9IAyN-8E","colab":{}},"source":["import _pickle as cPickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6q5GMbgg6WWk","colab":{}},"source":["# Load the inception v3 model\n","model = InceptionV3(weights='imagenet')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aS8YgzdL6jDc","colab":{}},"source":["# Create a new model, by removing the last layer (output layer) from the inception v3\n","model_new = Model(model.input, model.layers[-2].output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j1trHwqD6ncA","colab":{}},"source":["# Function to encode a given image into a vector of size (2048, )\n","def encode(image):\n","    image = preprocess(image) # preprocess the image\n","    fea_vec = model_new.predict(image) # Get the encoding vector for the image\n","    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n","    return fea_vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pU3Nts2b6uro","outputId":"c28dced5-2e4f-4d14-9ebf-ece929804144","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Call the funtion to encode all the train images\n","# This will take a while on CPU - Execute this only once\n","\n","start = time()\n","print(len(images))\n","encoding_train = {}\n","for img in train_img:\n","    encoding_train[img[len(images):]] = encode(img)\n","print(\"Time taken in seconds =\", time()-start)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["99\n","Time taken in seconds = 1098.705070734024\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tx0rJRdW6whP","colab":{}},"source":["\n","# Save the bottleneck train features to disk\n","with open(abs_path + \"pickel_files/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n","    cPickle.dump(encoding_train, encoded_pickle)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5t8D4AOvPzmM","outputId":"356a35c1-2567-4e97-a136-aaa064328539","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["start = time()\n","encoding_dev = {}\n","for img in dev_img:\n","    encoding_dev[img[len(images):]] = encode(img)\n","print(\"Time taken in seconds =\", time()-start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Time taken in seconds = 215.39680171012878\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"E4N2zFTfP-fS","colab":{}},"source":["# Save the bottleneck train features to disk\n","with open(abs_path + \"pickel_files/encoded_dev_images.pkl\", \"wb\") as encoded_pickle:\n","    cPickle.dump(encoding_dev, encoded_pickle)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2WVmfwP878YO","outputId":"ef208bfb-5579-4050-b63c-d08724676190","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Call the funtion to encode all the test images - Execute this only once\n","start = time()\n","encoding_test = {}\n","for img in test_img:\n","    encoding_test[img[len(images):]] = encode(img)\n","print(\"Time taken in seconds =\", time()-start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Time taken in seconds = 214.95038485527039\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KQyrKaPL8BnA","colab":{}},"source":["\n","# Save the bottleneck test features to disk\n","with open(abs_path + \"pickel_files/encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n","    cPickle.dump(encoding_test, encoded_pickle)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9Dk_x2Pt8PAO","outputId":"e5b14d3d-7d90-4728-b766-2fcf9b82d0a0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_features = load(open(abs_path + \"pickel_files/encoded_train_images.pkl\", \"rb\"))\n","print('Photos: train=%d' % len(train_features))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Photos: train=6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"teR1zKKvQq5a","outputId":"790a8c02-3564-4c2c-9559-6b67d6922797","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dev_features = load(open(abs_path + \"pickel_files/encoded_dev_images.pkl\", \"rb\"))\n","print('Photos: dev=%d' % len(dev_features))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Photos: dev=1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JFBM926AQzTS","outputId":"33f83238-37e8-4648-b474-c55d8a140df8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test_features = load(open(abs_path + \"pickel_files/encoded_test_images.pkl\", \"rb\"))\n","print('Photos: test=%d' % len(test_features))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Photos: test=1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bmcwJ7zUuWeN","colab_type":"text"},"source":["## Data Preprocessing of captions\n","During the training period, captions will be the target variables (Y) that the model is learning to predict.\n","But the prediction of the entire caption, given the image does not happen at once. We will predict the caption word by word. Thus, we need to encode each word into a fixed sized vector. However, this part will be seen later when we look at the model design, but for now we will create two Python Dictionaries namely “wordtoix” (pronounced — word to index) and “ixtoword” (pronounced — index to word).\n","\n","We will represent every unique word in the vocabulary by an integer (index). As seen above, we have 1652 unique words in the corpus and thus each word will be represented by an integer index between 1 to 1652.\n","These two Python dictionaries can be used as follows:\n","    wordtoix[‘abc’] -> returns index of the word ‘abc’\n","    ixtoword[k] -> returns the word whose index is ‘k’"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hARTCB-f8XHB","outputId":"223b66b8-1ac7-481b-e2ac-65b8596ea11b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Create a list of all the training captions\n","all_train_captions = []\n","for key, val in train_descriptions.items():\n","    for cap in val:\n","        all_train_captions.append(cap)\n","len(all_train_captions)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30000"]},"metadata":{"tags":[]},"execution_count":150}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0OJyNg5C8ZUF","outputId":"45ec4866-9650-4bf6-905c-b80ab2f7e90c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Consider only words which occur at least 10 times in the corpus\n","word_count_threshold = 10\n","word_counts = {}\n","nsents = 0\n","for sent in all_train_captions:\n","    nsents += 1\n","    for w in sent.split(' '):\n","        word_counts[w] = word_counts.get(w, 0) + 1\n","\n","vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["preprocessed words 7578 -> 1651\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HpAD1-RG8iuN","colab":{}},"source":["ixtoword = {}\n","wordtoix = {}\n","\n","ix = 1\n","for w in vocab:\n","    wordtoix[w] = ix\n","    ixtoword[ix] = w\n","    ix += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yT1EENgu8lCX","outputId":"d10bbc96-8816-4c14-82bd-2c075e94cc97","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocab_size = len(ixtoword) + 1 # one for appended 0's\n","vocab_size"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1652"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ftr9gXz-8n4g","outputId":"aadd3ebc-1a83-4593-d1d0-49beb5847f4f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# convert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n","\n","# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)\n","\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Description Length: 34\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5LXnKu7D-mO9","outputId":"15f5a68b-6bef-40f3-bf3a-ffeeb437060b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Load Glove vectors\n","glove_dir = glov_path\n","embeddings_index = {} # empty dictionary\n","f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'), encoding=\"utf-8\")\n","\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 400000 word vectors.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SD0zKe1f-psA","outputId":"ada9cf14-dcd7-4b43-861c-e9354fbd70c1","colab":{}},"source":["embedding_dim = 300\n","\n","# Get 300-dim dense vector for each of the 10000 words in out vocabulary\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","counter_glov = 0\n","for word, i in wordtoix.items():\n","    #if i < max_words:\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in the embedding index will be all zeros\n","        embedding_matrix[i] = embedding_vector\n","        counter_glov +=1\n","    else:\n","        print(word)\n","print(counter_glov)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["startseq\n","endseq\n","snowcovered\n","tshirt\n","waterskier\n","floaties\n","upsidedown\n","merrygoround\n","rollerblader\n","1642\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nI4j5ynC-uHl","outputId":"3cd4825b-6688-4505-c52e-85c89e3382a8","executionInfo":{"status":"error","timestamp":1559303643921,"user_tz":-120,"elapsed":846,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["embedding_matrix.shape"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ac7591c3ccf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"khmKgyF2WQTe","outputId":"7da5a8db-b62f-48fd-b145-6d5ddfc6a93b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["max_length # max sequence length of caption"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OjFULA2098W6"},"source":["### caption"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t9CIhwWsNKWA","outputId":"f592ed53-9125-41d4-999d-4e570ca6eb32","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(train_descriptions)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6000"]},"metadata":{"tags":[]},"execution_count":159}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sXmwL14zWZr8","outputId":"f55ea310-553c-4567-dab5-2fa3a2c6e71e","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["train_descriptions['990890291_afc72be141']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['startseq man does wheelie on his bicycle on the sidewalk endseq',\n"," 'startseq man is doing wheelie on mountain bike endseq',\n"," 'startseq man on bicycle is on only the back wheel endseq',\n"," 'startseq asian man in orange hat is popping wheelie on his bike endseq',\n"," 'startseq man on bicycle riding on only one wheel endseq']"]},"metadata":{"tags":[]},"execution_count":160}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jYdMqsPG9-EQ"},"source":["### features (vector) of images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g8600LU_NMVE","outputId":"54ac200b-1ed1-48cc-e4fe-af32fc280249","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(train_features)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6000"]},"metadata":{"tags":[]},"execution_count":161}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AI7QF4Yb91Z8","outputId":"1a533550-9b1f-4267-e2ee-0271781acc7c","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_features['990890291_afc72be141.jpg']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.00971348, 0.23514062, 0.3525491 , ..., 0.05749114, 0.9089296 ,\n","       0.36450088], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":162}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BnHc-qdVD7HX","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.utils.np_utils import to_categorical"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Il7759Oh-wVb"},"source":["### Model architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fD3m6NXJ-yMd","colab":{}},"source":["# image input\n","image_in = Input(shape=(2048,), name='image_inputs') # feature from inception net has dimension (, 2048)\n","fe1 = Dropout(0.5, name='dropout_img_feats')(image_in)\n","fe2 = Dense(300, activation='relu', name = 'dense_img_feats')(fe1) # reduce the dimension into 300 with FC projection\n","\n","# caption input\n","# as input for RNN, with dimension (, None) ; \n","# we set it into None since we will re-use the model it in inference stage (need a variable lengh of inputs)\n","cap_in = Input(shape=(None,),name='caption_inputs') \n","embed_cap = Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], trainable = False) # with dimension (, None, embedding_dim)\n","\n","se1 = embed_cap(cap_in)\n","drop_cap = Dropout(0.5)\n","se2 = drop_cap(se1)\n","lstm_layer = LSTM(300)\n","se3 = lstm_layer(se2)\n","\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(300, activation='relu')\n","dense_decoder = decoder2(decoder1)\n","pred_layer = Dense(vocab_size, activation='softmax')\n","outputs = pred_layer(dense_decoder)\n","\n","model = Model(inputs=[image_in, cap_in], outputs=outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"m21XnuwZgIy-","outputId":"a897353d-cdb0-4136-8a23-6511140a80c0","colab":{"base_uri":"https://localhost:8080/","height":493}},"source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","caption_inputs (InputLayer)     (None, None)         0                                            \n","__________________________________________________________________________________________________\n","image_inputs (InputLayer)       (None, 2048)         0                                            \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, None, 300)    495600      caption_inputs[0][0]             \n","__________________________________________________________________________________________________\n","dropout_img_feats (Dropout)     (None, 2048)         0           image_inputs[0][0]               \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, None, 300)    0           embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","dense_img_feats (Dense)         (None, 300)          614700      dropout_img_feats[0][0]          \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   (None, 300)          721200      dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 300)          0           dense_img_feats[0][0]            \n","                                                                 lstm_2[0][0]                     \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 300)          90300       add_2[0][0]                      \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1652)         497252      dense_3[0][0]                    \n","==================================================================================================\n","Total params: 2,419,052\n","Trainable params: 1,923,452\n","Non-trainable params: 495,600\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J3s3uWZLt-Bh"},"source":["### Compile model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1Zf0aJnStyUL","colab":{}},"source":["adam = optimizers.Adam(lr=0.001)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xLkISycuvXI7","colab":{}},"source":["model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OtEOH0z1G9pQ","colab":{}},"source":["# data generator, intended to be used in a call to model.fit_generator()\n","def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n","  \n","    X1, X2, y = list(), list(), list()\n","    n=0\n","    # loop for ever over images\n","    while 1:\n","        for key, desc_list in descriptions.items():\n","            n+=1\n","            # retrieve the photo feature\n","            photo = photos[key+'.jpg']\n","            for desc in desc_list:\n","                # encode the sequence\n","                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n","                # split one sequence into multiple X, y pairs\n","                for i in range(1, len(seq)):\n","                    # split into input and output pair\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input sequence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","                    # store\n","                    X1.append(photo)\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            # yield the batch data\n","            if n==num_photos_per_batch:\n","                yield [[array(X1), array(X2)], array(y)]\n","                X1, X2, y = list(), list(), list()\n","                n=0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pmjbORSmHGTK","colab":{}},"source":["epochs = 10\n","number_pics_per_batch = 3\n","steps = len(train_descriptions)//number_pics_per_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_KkMJGwLHIcj","colab":{}},"source":["for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_batch)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    model.save('./model_' + str(i) + '.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"66PPpa3mVfCf","colab":{}},"source":["for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_batch)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    model.save('./model_' + str(i) + '.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ctlh6XVowgfr","colab":{}},"source":["model.load_weights('./model_9.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D5qLJDZKlFat","colab":{}},"source":["def greedySearch(photo):\n","    in_text = 'startseq'\n","    for i in range(max_length):\n","        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n","        sequence = pad_sequences([sequence], maxlen=max_length)\n","        yhat = model.predict([photo,sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = ixtoword[yhat]\n","        in_text += ' ' + word\n","        if word == 'endseq':\n","            break\n","    final = in_text.split()\n","    final = final[1:-1]\n","    final = ' '.join(final)\n","    return final"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SmNSTuh9lGZj","outputId":"9b6603ba-5998-4171-91c9-7dd76b751bd2","colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["pic = list(test_features.keys())[0]\n","print(pic)\n","image = test_features[pic].reshape((1,2048))\n","# x=plt.imread(images+pic)\n","# plt.imshow(x)\n","# plt.show()\n","# print(\"Greedy:\",greedySearch(image))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1056338697_4f7d7ce270.jpg\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qS0leCFVwwbW","colab":{}},"source":["pic = list(test_features.keys())[30]\n","image = test_features[pic].reshape((1,2048))\n","x=plt.imread(images+pic)\n","plt.imshow(x)\n","plt.show()\n","print(\"Greedy:\",greedySearch(image))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eBAmhhSvqFZa","colab":{}},"source":["def create_decoder_model():\n","  \n","  # image features from encoder outputs\n","  enc_out = Input(shape=(300,), name='')\n","  \n","  in_decoder = Input(shape=(None, ))\n","  \n","  in_dec_embedded =  embed_cap(in_decoder)\n","  in_dec_embedded = drop_cap(in_dec_embedded)\n","  \n","  enc_cap = lstm_layer(in_dec_embedded)\n","  \n","  \n","  \n","  x_cap = Reshape((1, 300))(enc_cap)\n","  x_img = Reshape((1, 300))(enc_out)\n","  \n","  context_concat = add([x_cap, x_img])\n","\n","  \n","  s = decoder2(context_concat)\n","  \n","  softmax_prob = pred_layer(s)\n","  \n","  decoder_model = Model([in_decoder] + [enc_out]  , [softmax_prob] )\n","  \n","  return decoder_model\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6MrbQcMlywTz","colab":{}},"source":["encoder_model = Model(inputs=image_in, outputs=fe2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6D2qXUluyxNY","colab":{}},"source":["decoder_model = create_decoder_model()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CM-7RDGzyKAr","colab":{}},"source":["def greedy_generator(in_images):\n","  \n","  in_text = 'startseq'\n","  img_feats = encoder_model.predict(in_images)\n","\n","  decoded_text = ''\n","  \n","  for i in range(max_length):\n","    \n","    sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n","    sequence = pad_sequences([sequence], maxlen=max_length)\n","    yhat = decoder_model.predict([sequence, img_feats], verbose=0)\n","    yhat = np.argmax(yhat)\n","    word = ixtoword[yhat]\n","    in_text += ' ' + word\n","    if word == 'endseq':\n","        break\n","        \n","  final = in_text.split()\n","  final = final[1:-1]\n","  final = ' '.join(final)\n","  return final\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U2jYIjUmycEW","colab":{}},"source":["test_img_in = []\n","test_cap_true = []\n","img_keys = []\n","for key, desc_list in test_descriptions.items():\n","  \n","  img_vect = test_features[key+'.jpg'].reshape((1,2048))\n","    \n","  for desc in desc_list:\n","       \n","    test_img_in.append(img_vect)\n","    test_cap_true.append(desc)\n","    img_keys.append(key)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6CB5_odaydHJ","colab":{}},"source":["x=plt.imread(images+img_keys[50]+'.jpg')\n","plt.imshow(x)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NgmK1GqRyprS","colab":{}},"source":["greedy_generator(test_img_in[50])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wzEyVfef6sQ4","colab":{}},"source":["decoding_mode = BeamSearchDecoder(encoder_model=encoder_model, img_inputs=test_img_in[50], decoder_model=decoder_model, indices_words=ixtoword, words_indices=wordtoix, \\\n","                                  decoder_length=34, \\\n","                                  rnn_dim=300, beam_width=4, filepath='./')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uHheK6R26ydP","colab":{}},"source":["import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fVy7aJfdwtFI","colab":{}},"source":["decoding_mode.decode()"],"execution_count":0,"outputs":[]}]}