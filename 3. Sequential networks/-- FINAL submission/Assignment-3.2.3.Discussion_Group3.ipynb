{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "185oEPNrFM5H"
   },
   "source": [
    "## Assignment 3.2. Image Caption Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6tXAThWFM5P"
   },
   "source": [
    "## Task 2.3: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GISxmA_cYQEU"
   },
   "source": [
    "**1. Briefly describe one of the preceding works on modeling Image-Caption according to the paper and its limitation. Name the advantage(s) of the current Image-Caption generator as compared to the previous work?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xk2oWQAOYcvX"
   },
   "source": [
    "*Paper: G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011.*\n",
    "\n",
    "The approach is broadly divided into two stages: 1) content planning, where using computer vision algorithms specific object from genral images are identified, cleaned and collected to be used. 2) surface realization, where words are chosen to describe the images. The process starts with detecting individual objects from an image like tree, dogs etc. Each object detected are then described by attributes which describe the objects the best like 'green color' for a tree. Then the object and the attributes are correlated with the help of connecting words like prepositions e.g. the tree with green color. With the help of Condiitonal Random fields, all possible labels in terms of sentences are prepared in pairs or triples.Then a labelled and directed graph is generated ad the final sentence computed.\n",
    "\n",
    "***Advantages of the current approach:***\n",
    "\n",
    "It uses a 'single' network with multimodal embeddings to generate novel description of the images with no or very less human intervention. The advantage of using a single powerful network is that feeding images and their captions side by side enable the network to remmeber the caption correletade with that particular image.\n",
    "It is capable of describing or atleast mentioning unrelated objects in the test images, given the objects are in the training data but not particularly related with the main composition of the test image in the training image.\n",
    "It gives a way to evaulate the quality of the captions generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WL8MBF80FM5X"
   },
   "source": [
    "**2. Given an image with multiple text descriptions, how to represent this pair of data as model input-output?**\n",
    "\n",
    "- Explain the answer in a notation (x, y) or clear description that guarantees you can run the same model with your input-output representation. Name all the input(s) and output(s) of the model.\n",
    "\n",
    "- How does the model extract image features from raw images? Name and briefly explain how to employ the feature extractor. Include the dimension (array shape) of the extracted features.\n",
    "\n",
    "\n",
    "- How are the train descriptions represented into the model? Why do we need to add “starting” and “ending” token of every caption in a preprocessing stage?\n",
    "\n",
    "**Answer:** In caption generation, the caption is generated each word by word in the output in an iterative manner. The sequence of generated words in time step t-1 is again fed as input in time step t. Therefore a first word is required to kick off the generation process or use as input at time t = 0 for a particular image, as well as an end word to stop generation. This is why, the 2 words are used in the first and last of a caption.\n",
    "\n",
    "\n",
    "\n",
    "- How does the model initialize RNN states for caption generator? What is being fed into the decoder states in the first time step (t = 0)?\n",
    "\n",
    "**Answer:** The RNN states are initialized with the context embedded captions of the input image. The caption data are embedded using a pretrained dictionary in order to capture their correlation and context information. Then they are passed through an embedding layer of the vovabulary size of all the captions present and fed to the RNN.\n",
    "\n",
    "At time step t=0, the output from the RNN encoder and the encoded image are concatenated and fed to the decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nkGGi-sFM5s"
   },
   "source": [
    "**3. What is the motivation of incorporating Beam Search in Sequence-to-Sequence learning? Briefly explain how the method works in an inference stage.**\n",
    "\n",
    "Beam search is a heuristic method which returns a list of most likely output instead of one. The more the possibilities it considers, the chance of the sentence being translated is better. Greedy search always selects the one with the highest probablility without regarding the whole problem. It decides based on the information it has at one step. Beam search keeps in memory all the probable words based on the number of words decided by the user.\n",
    "\n",
    "Beam search starts with parameter k as input, which is the beam width or the number of probable words the algorithm will consider. At each step, each candidate sequence is expanded with all possible next steps. For each of those words, their respective scores are calculated by multiplying the probabilities. Then k sequences with most likely probabilities are are kept and resr of the candidates are eliminated. The process repeats until end of the sequence is reached."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Assignment-3.2.3.Discussion.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
