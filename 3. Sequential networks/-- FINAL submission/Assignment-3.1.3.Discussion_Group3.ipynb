{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Assignment-3.1.3.Discussion_Group3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZPlF3PF88PA"},"source":["## Assignment 3.1. Sequence Classification"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kv0uaIux88PB"},"source":["## Task 1.3: Discussion"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pQTg4OauWSxh"},"source":["**1.   What is the motivation of incorporating an \"attention mechanism\" in a Machine Translation task? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention.**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9BHcQevQ88PE"},"source":["\n","**Answer**\n","\n","The motivation of incorporating attention mechanism in machine translaton task is to pay visual attention to different parts of the image or correlate words in text. We, as human beings, are able to pay attention to specific parts of an image and label it without being asked definitively to do so. Machines on the other hand, if not directed, is prone to give equal importance to the whole image or text. In other words, attention can be defined as a vector of importance defined in weights. In order to infer an image or a word in the text, the weight vector is used to estimate how strongly the different points in an image or text correlate with the inference.\n","\n","The main issue of seq2seq model is that it uses a fixed length context vector which is inacapable of remembering long sentences. Once it completes the whole input, more often it has forgotten the first part. The attention mechanisn is used to resolve this. It helps the machine translation mechanism to memorize long sentences by creating shortcuts between the context vector and the source input, thus resolving the problem of remembering the whole context vector. The alignment between the source and target is learnt and controlled by the context vector, which is customizable for each output element\n","\n","Advantages of attention based models over models without attention are:\n","\n","\n","*   Improved performance on machine translate tasks\n","*   It allows models to learn alignments between different modalities (different data types) e.g. between visual features of a picture and its text description\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5IM6H9iFWZ0y"},"source":["**2.  Likewise, what is the motivation of adding an \"attention\" network in aspect-level sentiment classification? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quCxtS0oWcRH"},"source":["**Answer**\n","The purpose of adding attention in sentiment analysis is to focus on different parts of the sentence and extract different sentiments for different aspects. E.g. \"I love the color of the case but the size is too big\". The sentence has two aspects, color and size with two sentiments involved. Most approaches tend to find out the sentiment of the overall sentence regardless of entities mentioned. Attention based mechanism finds the correlation between the entities present and the associated polarity by focussing on key parts of the sentence.\n","\n","\n","With multiple entities presented in a single sentence, attention is trying to solve the same 'forgetting' problem for aspects present in the initial part of the sentence by focussing only on the key parts correlated with the aspect and important for finding out the polarity associated with that aspect.\n","\n","Advantages:\n","*   In the non-attention based models, source representation x is used only once to initialize the decoder hidden state, whereas in attention-based networks, a set of source hidden states are throughout the translation process. This, along with the context vector helps the network remember and focus on what is required to get the target output, which improves the performance without requiring any domain knowledge and more input data."]}]}