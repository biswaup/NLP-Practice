{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_2_Jose.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uj4T8PEHGbMF"},"source":["# Assignment 2\n","## Question 1: Siamese networks & one-shot learning (7pt)\n","The Cifar-100 dataset is similar to the Cifar-10 dataset. It also consists of 60,000 32x32 RGB images, but they are distributed over 100 classes instead of 10. Thus, each class has much fewer examples, only 500 training images and 100 testing images per class. For more info about the dataset, see https://www.cs.toronto.edu/~kriz/cifar.html.\n","\n","*HINT: Import the Cifar-100 dataset directly from Keras, no need to download it from the website. Use* `label_mode=\"fine\"`\n","\n","### Task 1.1: Siamese network\n","**a)**\n","* Train a Siamese Network on the first 80 classes of (the training set of) Cifar-100, i.e. let the network predict the probability that two input images are from the same class. Use 1 as a target for pairs of images from the same class (positive pairs), and 0 for pairs of images from different classes (negative pairs). Randomly select image pairs from Cifar-100, but make sure you train on as many positive pairs as negative pairs.\n","\n","* Evaluate the performance of the network on 20-way one-shot learning tasks. Do this by generating 250 random tasks and obtain the average accuracy for each evaluation round. Use the remaining 20 classes that were not used for training. The model should perform better than random guessing.\n","\n","For this question you may ignore the test set of Cifar-100; it suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n","\n","*HINT: First sort the data by their labels (see e.g.* `numpy.argsort()`*), then reshape the data to a shape of* `(n_classes, n_examples, width, height, depth)`*, similar to the Omniglot data in Practical 4. It is then easier to split the data by class, and to sample positive and negative images pairs for training the Siamese network.*\n","\n","*NOTE: do not expect the one-shot accuracy for Cifar-100 to be similar to that accuracy for Omniglot; a lower accuracy can be expected. However, accuracy higher than random guess is certainly achievable.*"]},{"cell_type":"code","metadata":{"id":"VVPaVGzHb8gz","colab_type":"code","outputId":"b45416f5-a21a-4df6-8044-cd14c55669ca","executionInfo":{"status":"ok","timestamp":1558390474270,"user_tz":-120,"elapsed":1805,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import numpy as np\n","import keras\n","import tensorflow as tf\n","from keras.models import Model, Sequential\n","from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization, concatenate\n","from keras.regularizers import l2\n","from keras.utils import to_categorical\n","from keras import backend as K\n","from keras.losses import binary_crossentropy\n","from keras.callbacks import EarlyStopping\n","from keras.optimizers import Adam\n","import os\n","import matplotlib.pyplot as plt\n","import pickle\n","from sklearn.utils import shuffle\n","from itertools import permutations\n","import random \n","from keras.datasets import cifar100\n","plt.style.use('ggplot')\n","%matplotlib inline"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"c4Zh0nUkk7yy","colab_type":"text"},"source":["### Load the CIFAR-100 data and create training and test set\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MaoAaEv17v6k","outputId":"083a9830-1c58-40df-852d-b0fc6baa3136","executionInfo":{"status":"ok","timestamp":1558390515734,"user_tz":-120,"elapsed":38819,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["# Extract from Cifar-100 train and test\n","(x_cifar_train, y_cifar_train),(x_cifar_test,y_cifar_test) = cifar100.load_data(label_mode='fine')\n","\n","print('x_cifar_train shape: ', x_cifar_train.shape)\n","print('y_cifar_train shape: ', y_cifar_train.shape)\n","\n","# We retrieve the ordered indexes to create our train set with the first 80 classes and test set with the last 20 classes\n","ordered_indexes = np.argsort(y_cifar_train.flatten())\n","train_indexes = ordered_indexes[:int(0.8*len(ordered_indexes))]\n","test_indexes = ordered_indexes[int(0.8*len(ordered_indexes)):]\n","\n","# x_train, x_test, y_train, y_test\n","x_train = x_cifar_train[train_indexes]\n","x_test = x_cifar_train[test_indexes]\n","y_train = y_cifar_train[train_indexes]\n","y_test = y_cifar_train[test_indexes]\n","print('Class ranges of training: %d to %d' % (int(y_train[0]), int(y_train[-1])))\n","print('Class ranges of testing: %d to %d' % (int(y_test[0]), int(y_test[-1])))\n","\n","# Reshape as (n_class, n_example, width, height, depth)\n","x_train = np.reshape(x_train, (80,-1,32,32,3))\n","y_train = np.reshape(y_train, (80,-1,1))\n","x_test = np.reshape(x_test, (20,-1,32,32,3))\n","y_test = np.reshape(y_test, (20,-1,1))\n","\n","print('\\nx_train shape: ', x_train.shape)\n","print('x_test shape: ', x_test.shape)\n","print('y_train shape: ', y_train.shape)\n","print('y_test shape: ', y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n","169009152/169001437 [==============================] - 36s 0us/step\n","x_cifar_train shape:  (50000, 32, 32, 3)\n","y_cifar_train shape:  (50000, 1)\n","Class ranges of training: 0 to 79\n","Class ranges of testing: 80 to 99\n","\n","x_train shape:  (80, 500, 32, 32, 3)\n","x_test shape:  (20, 500, 32, 32, 3)\n","y_train shape:  (80, 500, 1)\n","y_test shape:  (20, 500, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Bk5Rcz514Yi","colab_type":"text"},"source":["### Create batch of pair images and target (balanced dataset)"]},{"cell_type":"code","metadata":{"id":"Wf2p7PYxc4og","colab_type":"code","colab":{}},"source":["def get_batch(batch_size, X):\n","    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n","    n_classes, n_examples, w, h, d = X.shape\n","    # randomly sample several classes to use in the batch\n","    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n","    # initialize 2 empty arrays for the input image batch\n","    pairs = [np.zeros((batch_size, h, w, d)) for i in range(2)]\n","    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n","    targets = np.zeros((batch_size,))\n","    targets[batch_size//2:] = 1\n","    for i in range(batch_size):\n","        category = categories[i]\n","        idx_1 = np.random.randint(0, n_examples)\n","        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n","        idx_2 = np.random.randint(0, n_examples)\n","        # Last part of Batch array: pick images of same class for 1st half, different for 2nd\n","        if i >= batch_size // 2:\n","            category_2 = category\n","        # First part of Batch array\n","        else:\n","            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n","            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n","        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, d)\n","    return pairs, targets"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RNL4hfl2pAy","colab_type":"text"},"source":["### Siamese network"]},{"cell_type":"code","metadata":{"id":"eEtiueVq2k8O","colab_type":"code","outputId":"d045dd39-07bb-406d-b73c-4a059e4463e7","executionInfo":{"status":"ok","timestamp":1558390606270,"user_tz":-120,"elapsed":3557,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":997}},"source":["input_shape = (32, 32, 3)\n","left_input = Input(input_shape)\n","right_input = Input(input_shape)\n","\n","# build convnet to use in each siamese 'leg'\n","convnet = Sequential()\n","convnet.add(Conv2D(64, (5,5), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n","convnet.add(MaxPooling2D())\n","convnet.add(BatchNormalization())\n","convnet.add(Dropout(0.25))\n","\n","convnet.add(Conv2D(128, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n","convnet.add(MaxPooling2D())\n","convnet.add(BatchNormalization())\n","convnet.add(Dropout(0.25))\n","\n","convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n","#convnet.add(MaxPooling2D())\n","#convnet.add(BatchNormalization())\n","#convnet.add(Dropout(0.25))\n","\n","#convnet.add(Conv2D(256, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n","convnet.add(Flatten())\n","#convnet.add(BatchNormalization())\n","#convnet.add(Dropout(0.25))\n","convnet.add(Dense(1152, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n","convnet.summary()\n","\n","# encode each of the two inputs into a vector with the convnet\n","encoded_l = convnet(left_input)\n","encoded_r = convnet(right_input)\n","\n","# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n","L1_distance = lambda x: K.abs(x[0]-x[1])\n","both = Lambda(L1_distance)([encoded_l, encoded_r])\n","prediction = Dense(1, activation='sigmoid')(both)\n","siamese_net = Model(inputs=[left_input,right_input], outputs=prediction)\n","\n","\n","siamese_net.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n","\n","siamese_net.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 28, 28, 64)        4864      \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 11, 11, 128)       131200    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 5, 5, 128)         0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 5, 5, 128)         512       \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 5, 5, 128)         0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 3, 3, 128)         147584    \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 1152)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1152)              1328256   \n","=================================================================\n","Total params: 1,612,672\n","Trainable params: 1,612,288\n","Non-trainable params: 384\n","_________________________________________________________________\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","sequential_1 (Sequential)       (None, 1152)         1612672     input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 1152)         0           sequential_1[1][0]               \n","                                                                 sequential_1[2][0]               \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 1)            1153        lambda_1[0][0]                   \n","==================================================================================================\n","Total params: 1,613,825\n","Trainable params: 1,613,441\n","Non-trainable params: 384\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zXs50g9nq4iz","colab_type":"text"},"source":["### Batch generator and Train function\n"]},{"cell_type":"code","metadata":{"id":"G5QF5nQ4Kkex","colab_type":"code","colab":{}},"source":["def batch_generator(batch_size, X):\n","    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n","    while True:\n","        pairs, targets = get_batch(batch_size, X)\n","        yield (pairs, targets)\n","\n","def train(model, X_train, batch_size=64, steps_per_epoch=100, epochs=1, verbose=1):\n","    model.fit_generator(batch_generator(batch_size, x_train), steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=verbose)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m05I3csjtYn6","colab_type":"text"},"source":["### One-shot functions\n"]},{"cell_type":"code","metadata":{"id":"qZU2QQ-Itmnh","colab_type":"code","colab":{}},"source":["# N number of 20-oneshot tasks, k number of random tasks 250\n","\n","def make_oneshot_task(N, X, Y):\n","    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n","    \"\"\" Just the first index 0 will have the true (both images coming from the same class)!! \"\"\"\n","    n_classes, n_examples, w, h, d = X.shape\n","    \n","    # choose 20-random indexes from the 500 examples\n","    indices = np.random.randint(0, n_examples, size=(N,))\n","    \n","    # choose 20-random - since replace is False the index 0 will contain the True Unique category\n","    categories = np.random.choice(range(n_classes), size=(N,), replace=False)\n","    \n","    # we choose the category of the first index as the true one\n","    true_category = categories[0]\n","    \n","    # choose 2 random indexes for 2 examples\n","    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n","    \n","    # test_image.shape = (20, 32, 32, 3) This is a set of 20 images of the test_image\n","    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n","    \n","    # support set.shape = (20,32,32,3) This is a set of 20 images of random categories.\n","    support_set = X[categories, indices, :, :]\n","    \n","    # modify the first element of support set with ex2\n","    support_set[0, :, :] = X[true_category, ex2]\n","    support_set = support_set.reshape(N, w, h, d)\n","    \n","    # all targets are fitted with 0 (since is randomized), just the first example is true \n","    targets = np.zeros((N,))\n","    targets[0] = 1\n","    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n","    \n","    # shuffled pair where just one pair of images have target 1 (belonging to same class)\n","    pairs = [test_image, support_set]\n","    return pairs, targets\n","\n","def test_oneshot(model, X, Y, N=20, k=250, verbose=True):\n","    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n","    # number of correct tasks\n","    n_correct = 0\n","    if verbose:\n","        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n","    for i in range(k):\n","        # inputs is the pair of images\n","        inputs, targets = make_oneshot_task(N, X, Y)\n","        \n","        # in probs we store the predictions \n","        probs = model.predict(inputs)\n","        \n","        # retrieve the index where is the maximum probability and compare it with the index of targets\n","        if np.argmax(probs) == np.argmax(targets):\n","            n_correct += 1\n","          \n","    percent_correct = (100.0*n_correct / k)\n","    if verbose:\n","        print(\"Got an average of {}% accuracy for {}-way one-shot learning during this loop\".format(percent_correct, N))\n","    return percent_correct"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXV68_wFrgoW","colab_type":"code","outputId":"f24bd0dd-2807-42a5-e770-ed6f3518c2f4","executionInfo":{"status":"ok","timestamp":1558205013179,"user_tz":-120,"elapsed":25422,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":988}},"source":["# Train and Test the architecture\n","loops = 10\n","best_acc = 0\n","epochs = 1\n","for i in range(loops):\n","    print(\"=== Training loop {} ===\".format(i+1))\n","    train(siamese_net, x_train, epochs=epochs)\n","    test_acc = test_oneshot(siamese_net, x_test, y_test)\n","    if test_acc >= best_acc:\n","        print(\"New best one-shot accuracy of {}%, saving model ...\".format(test_acc))\n","        siamese_net.save(\"siamese_cifar100.h5\")\n","        best_acc = test_acc\n","        \n","print('\\nBest accuracy obtained during whole training {}%.'.format(best_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=== Training loop 1 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 18ms/step - loss: 0.6300\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 13.6% accuracy for 20-way one-shot learning during this loop\n","New best one-shot accuracy of 13.6%, saving model ...\n","=== Training loop 2 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6338\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 18.4% accuracy for 20-way one-shot learning during this loop\n","New best one-shot accuracy of 18.4%, saving model ...\n","=== Training loop 3 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6358\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 18.0% accuracy for 20-way one-shot learning during this loop\n","=== Training loop 4 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6402\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 20.8% accuracy for 20-way one-shot learning during this loop\n","New best one-shot accuracy of 20.8%, saving model ...\n","=== Training loop 5 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6266\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 17.2% accuracy for 20-way one-shot learning during this loop\n","=== Training loop 6 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6405\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 18.4% accuracy for 20-way one-shot learning during this loop\n","=== Training loop 7 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6370\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 16.4% accuracy for 20-way one-shot learning during this loop\n","=== Training loop 8 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6273\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 18.4% accuracy for 20-way one-shot learning during this loop\n","=== Training loop 9 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 16ms/step - loss: 0.6338\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 21.2% accuracy for 20-way one-shot learning during this loop\n","New best one-shot accuracy of 21.2%, saving model ...\n","=== Training loop 10 ===\n","Epoch 1/1\n","100/100 [==============================] - 2s 17ms/step - loss: 0.6287\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 19.6% accuracy for 20-way one-shot learning during this loop\n","\n","Best accuracy obtained during whole training 21.2%.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"COiAqXWDAgCe"},"source":["***\n","\n","**b)** Compare the performance of your Siamese network for Cifar-100 to the Siamese network from Practical 4 for Omniglot. Name three fundamental differences between the Cifar-100 and Omniglot datasets. How do these differences influence the difference in one-shot accuracy?\n","\n","**Answer:**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IIHkoQ0PBWuB"},"source":["\n","Performance comparison:\n","\n","* Siamese network for Omniglot: 42.4% one-shot learning accuracy. The loss is in a range of 1 to 4.\n","* Siamese network for Cifar-100: 21.2% one-shot learning accuracy. The loss is  much lesser than on omniglot.\n","\n","\n","The omniglot dataset has 50 alphabets with 1623 handwritten characters. Each of the characters has 20 different samples. The CIFAR 100 dataset has 100 classes with 20 super classes and 5 sub classes per super class. Per superclass, there are 600 images.\n","The fundamental differences between the 2 datasets could be:\n","\n","\n","1.   CIFAR images are really small with 32*32*1 dimensions while the omniglot images are bigger with 105*105*1 dimensions. Greyscale images as omniglot with bigger dimensions are better to train a model than RGB images with smaller dimensions, which make them ambiguous.\n","2.   Some CIFAR images do not represent the subject fully, sometimes half of the subject is present in the image, while omniglot images are clear and big. As per models implemented by different authors, even human identification system could not reach an accuracy og 100% because some of the images are identified based on the context in the picture rather thean the whole subject present. Identifying the actual image by understanding the context is not a part of the model trained.\n","\n","The above 2 factors, along with very less samples belonging to 100 finer classes contribute towards low accuracy for CIFAR 100 dataset.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VWpFF_5-Bf4B"},"source":["***\n","\n","### Task 1.2: One-shot learning with neural codes\n","**a)**\n","* Train a CNN classifier on the first 80 classes of Cifar-100. Make sure it achieves at least 40% classification accuracy on those 80 classes (use the test set to validate this accuracy).\n","* Then use neural codes from one of the later hidden layers of the CNN with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction."]},{"cell_type":"code","metadata":{"id":"Cosxhv6-o2DF","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XaDW4RRXgdhw","colab_type":"code","outputId":"897d108f-73ca-48c6-eb68-4ff88dfaff08","executionInfo":{"status":"ok","timestamp":1558392848896,"user_tz":-120,"elapsed":1293,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"source":["# CNN architecture\n","\n","n_classes = 100\n","input_shape = (32,32,3)\n","input_image = Input(input_shape)\n","\n","cnn = Sequential()\n","\n","cnn.add(Conv2D(32, kernel_size=(4, 4), activation='sigmoid', input_shape=input_shape))\n","cnn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn.add(BatchNormalization())\n","cnn.add(Dropout(0.5))\n","\n","cnn.add(Conv2D(64, kernel_size=(3, 3), activation='sigmoid'))\n","cnn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn.add(BatchNormalization())\n","cnn.add(Dropout(0.5))\n","\n","cnn.add(Conv2D(128, kernel_size=(2, 2), activation='sigmoid'))\n","cnn.add(MaxPooling2D(pool_size=(2, 2)))\n","cnn.add(BatchNormalization())\n","cnn.add(Dropout(0.5))\n","\n","cnn.add(Flatten())\n","cnn.add(Dense(5000, activation='sigmoid'))\n","cnn.add(BatchNormalization())\n","cnn.add(Dropout(0.5))\n","cnn.add(Dense(n_classes, activation='softmax'))\n","\n","cnn.summary()\n","\n","# encode each of the two inputs into a vector with the convnet\n","encoded_image = cnn(input_image)\n","\n","cnn.compile(optimizer=Adam(lr=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_4 (Conv2D)            (None, 29, 29, 32)        1568      \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 14, 14, 32)        128       \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 12, 12, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 6, 6, 64)          256       \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 5, 5, 128)         32896     \n","_________________________________________________________________\n","max_pooling2d_6 (MaxPooling2 (None, 2, 2, 128)         0         \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 2, 2, 128)         512       \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 2, 2, 128)         0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 512)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 5000)              2565000   \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 5000)              20000     \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 5000)              0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 100)               500100    \n","=================================================================\n","Total params: 3,138,956\n","Trainable params: 3,128,508\n","Non-trainable params: 10,448\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-gdtoUlMgzmd","colab_type":"code","outputId":"a49b9c6d-2b1b-4db8-de12-c53a6717d1d9","executionInfo":{"status":"ok","timestamp":1558390661754,"user_tz":-120,"elapsed":1143,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Reshaping x_train, x_test, y_train, y_test\n","\n","x_train_reshaped = x_train.reshape(40000,32,32,3)\n","y_train_reshaped = y_train.reshape(40000,1)\n","y_train_reshaped = to_categorical(y_train_reshaped, 100)\n","\n","x_test_reshaped = x_test.reshape(10000,32,32,3)\n","y_test_reshaped = y_test.reshape(10000,1)\n","y_test_reshaped = to_categorical(y_test_reshaped, 100)\n","\n","x_train_reshaped = x_train_reshaped.astype('float32')\n","x_train_reshaped /= 255\n","\n","x_test_reshaped = x_test_reshaped.astype('float32')\n","x_test_reshaped /= 255\n","\n","\n","print('X train reshaped shape: ', x_train_reshaped.shape)\n","print('Y train reshaped shape: ',y_train_reshaped.shape)\n","print('X test reshaped shape: ',x_test_reshaped.shape)\n","print('Y test reshaped shape: ',y_test_reshaped.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["X train reshaped shape:  (40000, 32, 32, 3)\n","Y train reshaped shape:  (40000, 100)\n","X test reshaped shape:  (10000, 32, 32, 3)\n","Y test reshaped shape:  (10000, 100)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1b7izh4M-Oic","colab_type":"code","outputId":"e53c912c-2f58-49e2-d729-180b2db0273f","executionInfo":{"status":"ok","timestamp":1558392992601,"user_tz":-120,"elapsed":139330,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":1101}},"source":["history = cnn.fit(x=x_train_reshaped, y=y_train_reshaped, batch_size=100, epochs=30, validation_split=0.1, callbacks=[EarlyStopping(monitor='loss', patience=1)])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 36000 samples, validate on 4000 samples\n","Epoch 1/30\n","36000/36000 [==============================] - 6s 163us/step - loss: 5.5638 - acc: 0.0829 - val_loss: 10.6586 - val_acc: 0.0000e+00\n","Epoch 2/30\n","36000/36000 [==============================] - 4s 123us/step - loss: 4.5590 - acc: 0.1212 - val_loss: 12.1193 - val_acc: 0.0000e+00\n","Epoch 3/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 4.1981 - acc: 0.1421 - val_loss: 12.6935 - val_acc: 0.0000e+00\n","Epoch 4/30\n","36000/36000 [==============================] - 4s 123us/step - loss: 3.9392 - acc: 0.1630 - val_loss: 12.7349 - val_acc: 0.0000e+00\n","Epoch 5/30\n","36000/36000 [==============================] - 5s 126us/step - loss: 3.6850 - acc: 0.1886 - val_loss: 12.9947 - val_acc: 0.0000e+00\n","Epoch 6/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 3.5125 - acc: 0.2060 - val_loss: 12.5918 - val_acc: 0.0000e+00\n","Epoch 7/30\n","36000/36000 [==============================] - 5s 128us/step - loss: 3.3459 - acc: 0.2198 - val_loss: 12.7745 - val_acc: 0.0000e+00\n","Epoch 8/30\n","36000/36000 [==============================] - 5s 135us/step - loss: 3.2147 - acc: 0.2383 - val_loss: 12.8849 - val_acc: 0.0000e+00\n","Epoch 9/30\n","36000/36000 [==============================] - 5s 132us/step - loss: 3.1011 - acc: 0.2548 - val_loss: 12.6018 - val_acc: 0.0000e+00\n","Epoch 10/30\n","36000/36000 [==============================] - 4s 123us/step - loss: 2.9937 - acc: 0.2687 - val_loss: 13.2739 - val_acc: 0.0000e+00\n","Epoch 11/30\n","36000/36000 [==============================] - 4s 124us/step - loss: 2.8824 - acc: 0.2844 - val_loss: 13.1294 - val_acc: 0.0000e+00\n","Epoch 12/30\n","36000/36000 [==============================] - 4s 124us/step - loss: 2.8360 - acc: 0.2894 - val_loss: 12.9514 - val_acc: 0.0000e+00\n","Epoch 13/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 2.7464 - acc: 0.3041 - val_loss: 13.3121 - val_acc: 0.0000e+00\n","Epoch 14/30\n","36000/36000 [==============================] - 4s 124us/step - loss: 2.6792 - acc: 0.3139 - val_loss: 13.3446 - val_acc: 0.0000e+00\n","Epoch 15/30\n","36000/36000 [==============================] - 5s 135us/step - loss: 2.6303 - acc: 0.3221 - val_loss: 12.9507 - val_acc: 0.0000e+00\n","Epoch 16/30\n","36000/36000 [==============================] - 5s 138us/step - loss: 2.5821 - acc: 0.3284 - val_loss: 13.3949 - val_acc: 0.0000e+00\n","Epoch 17/30\n","36000/36000 [==============================] - 5s 127us/step - loss: 2.5416 - acc: 0.3404 - val_loss: 13.2125 - val_acc: 0.0000e+00\n","Epoch 18/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 2.4998 - acc: 0.3465 - val_loss: 13.2857 - val_acc: 0.0000e+00\n","Epoch 19/30\n","36000/36000 [==============================] - 4s 124us/step - loss: 2.4710 - acc: 0.3526 - val_loss: 13.3935 - val_acc: 0.0000e+00\n","Epoch 20/30\n","36000/36000 [==============================] - 4s 124us/step - loss: 2.4440 - acc: 0.3579 - val_loss: 13.4111 - val_acc: 0.0000e+00\n","Epoch 21/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 2.4122 - acc: 0.3621 - val_loss: 13.6834 - val_acc: 0.0000e+00\n","Epoch 22/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 2.3965 - acc: 0.3661 - val_loss: 13.7817 - val_acc: 0.0000e+00\n","Epoch 23/30\n","36000/36000 [==============================] - 4s 123us/step - loss: 2.3639 - acc: 0.3709 - val_loss: 13.7470 - val_acc: 0.0000e+00\n","Epoch 24/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 2.3483 - acc: 0.3765 - val_loss: 13.8102 - val_acc: 0.0000e+00\n","Epoch 25/30\n","36000/36000 [==============================] - 5s 131us/step - loss: 2.3244 - acc: 0.3778 - val_loss: 14.0054 - val_acc: 0.0000e+00\n","Epoch 26/30\n","36000/36000 [==============================] - 5s 134us/step - loss: 2.3121 - acc: 0.3829 - val_loss: 14.1445 - val_acc: 0.0000e+00\n","Epoch 27/30\n","36000/36000 [==============================] - 5s 128us/step - loss: 2.2939 - acc: 0.3867 - val_loss: 14.3121 - val_acc: 0.0000e+00\n","Epoch 28/30\n","36000/36000 [==============================] - 4s 125us/step - loss: 2.2620 - acc: 0.3917 - val_loss: 14.4713 - val_acc: 0.0000e+00\n","Epoch 29/30\n","36000/36000 [==============================] - 4s 122us/step - loss: 2.2489 - acc: 0.3984 - val_loss: 14.4270 - val_acc: 0.0000e+00\n","Epoch 30/30\n","36000/36000 [==============================] - 4s 123us/step - loss: 2.2340 - acc: 0.3999 - val_loss: 14.9026 - val_acc: 0.0000e+00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jkstdMsG-n6D","colab_type":"code","outputId":"0d76b215-e03f-43e8-ad4a-205f7d56c8a4","executionInfo":{"status":"ok","timestamp":1558393671975,"user_tz":-120,"elapsed":2005,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Loss and Test accuracy\n","loss, accuracy = cnn.evaluate(x_test_reshaped, y_test_reshaped, verbose=0)\n","\n","print('Test loss:', loss)\n","print('Test accuracy:', accuracy)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test loss: 14.752160887145996\n","Test accuracy: 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qSgI8fGElHqk","colab_type":"code","outputId":"325d8850-d18c-4988-ebfe-2f6670786af3","executionInfo":{"status":"ok","timestamp":1558393124890,"user_tz":-120,"elapsed":661,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":317}},"source":["## Printing the Loss\n","%matplotlib inline\n","plt.plot(history.history['loss'])\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Categorical CrossEntropy Loss')\n","plt.title('Loss Over Time')\n","plt.legend(['Train'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f48f4d0e780>"]},"metadata":{"tags":[]},"execution_count":64},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMWlZgjDGGJOWJQhjjDFpWYIwxhiT1v8HwAO1y09lJZAAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"b_CL8WYqY4-p","colab_type":"code","outputId":"9eaaef63-6dd0-4c5c-e9df-818c3c387876","executionInfo":{"status":"ok","timestamp":1558393109639,"user_tz":-120,"elapsed":701,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":317}},"source":["# Printing the Accuracy\n","plt.plot(history.history['acc'])\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Accuracy Over Time')\n","plt.legend(['Train'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f48f57459b0>"]},"metadata":{"tags":[]},"execution_count":63},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAY4AAAEaCAYAAAAG87ApAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxKeqN3aHrScAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"3M2BZ5F3Kadt","colab_type":"text"},"source":["## Neural codes\n"]},{"cell_type":"markdown","metadata":{"id":"Y4iqFlssxflI","colab_type":"text"},"source":["We ran several experiments extracting the encoding from different hidden layers and below it can be observed the 20 task one-shot accuracy reached.\n","\n","* Accuracy 9.2% with Layer 5\n","* Accuracy 6.8% with Layer 6\n","* Accuracy 7.2% with Layer 7\n","* Accuracy 7.2% with Layer 8\n","* Accuracy 8.8% with Layer 9\n","* Accuracy 8% with Layer 10\n","* Accuracy 4.4% with Layer 11\n","* Accuracy 7.6% with Layer 12\n","* Accuracy 6% / 6.8%  with Layer 13\n","* Accuracy 9.2% / 6.4% with Layer 14\n","* Accuracy 6.8% / 7.6%  with Layer 15\n","\n"]},{"cell_type":"code","metadata":{"id":"UtCGJOffNq_1","colab_type":"code","outputId":"820cbb89-b407-44a7-b1e6-28186162d155","executionInfo":{"status":"ok","timestamp":1558393174552,"user_tz":-120,"elapsed":626,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# L is the layer where we will extract the encoded state\n","N = 20;  L = 14\n","\n","outputs = [layer.output for layer in cnn.layers]  \n","print('Amount of stacked layers of the CNN: ', len(outputs))\n","\n","encoded_state = K.function(\n","[cnn.layers[0].input],  # we will feed the function with the input of the first layer\n","[cnn.layers[L].output,] # we want to get the output of the second layer\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Amount of stacked layers of the CNN:  17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"96Ui6p-2KhN6","colab_type":"code","outputId":"0fb4a0c1-3ca6-4d55-d58a-0d351886122e","executionInfo":{"status":"ok","timestamp":1558393319531,"user_tz":-120,"elapsed":141676,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":621}},"source":["def test_oneshot_cnn(X, Y, N=20, k=250, verbose=True):\n","    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n","    \n","    # number of correct tasks\n","    #print('Number of Layers to extract the encoding: %d \\n\\n' % L)\n","    n_correct = 0\n","    if verbose:\n","        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n","    \n","    for i in range(k):\n","        # inputs is the pair of images\n","        inputs, targets = make_oneshot_task(N, X, Y)\n","        \n","        # test_image we just take one since the rest of the array is duplicated => shape is (1,32,32,3)\n","        test_image = inputs[0][0].reshape(1, 32,32,3)\n","        support_set = inputs[1]\n","        \n","        # we initialize min_distance variable\n","        min_distance = 1000000\n","        distance_list = []\n","        \n","        # for loop over support_set array\n","        for i in range(0, support_set.shape[0]):\n","            support_image = support_set[i].reshape(1,32,32,3)\n","            target = targets[i].reshape(1)\n","\n","            # extract neural code from layer L\n","            neural_code_test_image = np.asarray(encoded_state([test_image]))\n","            neural_code_support_image = np.asarray(encoded_state([support_image]))\n","\n","            # Compute the L2 distance\n","            distance = np.linalg.norm(neural_code_test_image - neural_code_support_image)\n","            distance_list.append(distance)\n","\n","            # update the min_distance\n","            if(distance < min_distance):\n","                min_distance = distance\n","\n","        # compare if the index where the distance is minimum corresponds to the index where the target is 1\n","        if np.argmax(distance) == np.argmax(targets):\n","            n_correct += 1\n","    percent_correct = (100.0*n_correct / k)\n","    if verbose:\n","        print(\"Got an average of {}% accuracy for {}-way one-shot learning during this loop\".format(percent_correct, N))\n","    return percent_correct\n","\n","\n","# Train and Test the architecture\n","loops = 10\n","best_acc = 0\n","epochs = 1\n","for i in range(loops):\n","    print(\"=== Loop {} ===\".format(i+1))\n","    # Call test_oneshot_cnn functions\n","    test_acc = test_oneshot_cnn(x_test, y_test)\n","    if test_acc >= best_acc:\n","        print(\"New best one-shot accuracy of {}%  ...\".format(test_acc))\n","        best_acc = test_acc\n","        \n","print('\\nBest accuracy obtained during whole training {}%.'.format(best_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=== Loop 1 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 8.0% accuracy for 20-way one-shot learning during this loop\n","New best one-shot accuracy of 8.0%  ...\n","=== Loop 2 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 1.6% accuracy for 20-way one-shot learning during this loop\n","=== Loop 3 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 4.8% accuracy for 20-way one-shot learning during this loop\n","=== Loop 4 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 5.6% accuracy for 20-way one-shot learning during this loop\n","=== Loop 5 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 5.6% accuracy for 20-way one-shot learning during this loop\n","=== Loop 6 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 5.2% accuracy for 20-way one-shot learning during this loop\n","=== Loop 7 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 4.8% accuracy for 20-way one-shot learning during this loop\n","=== Loop 8 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 6.4% accuracy for 20-way one-shot learning during this loop\n","=== Loop 9 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 8.0% accuracy for 20-way one-shot learning during this loop\n","New best one-shot accuracy of 8.0%  ...\n","=== Loop 10 ===\n","Evaluating model on 250 random 20-way one-shot learning tasks ...\n","Got an average of 4.0% accuracy for 20-way one-shot learning during this loop\n","\n","Best accuracy obtained during whole training 8.0%.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M1BDzdPAz26B"},"source":["***\n","\n","**b)** Briefly motivate your CNN architecture, and discuss the difference in one-shot accuracy between the Siamese network approach and the CNN neural codes approach.\n","\n","**Answer:**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oRpVm956FR8P"},"source":["* For building the CNN architecture, we tried to use the one provided\n","in CIFAR-10, however we didn't succeed in having a good result with it.\n","This is mainly because of the less amount of examples per class. While\n","in CIFAR-10 we just have 10 classes and 5000 samples for training and\n","1000 samples for testing per class, in CIFAR-100 we have 100 classes and\n","just 500 samples for training and 100 for testing.\n","\n","* The accuracy of the classifier reaches 40% but the validation accuracy is always 0%.\n","If we decrease the validation split (0.005) we have validation accuracy above 0%, however\n","the behaviour of this loss looks random. That's why when we compute the test accuracy\n","we have 0%.\n","\n","* At one point, when adding more layers, we could contain the validation loss and reaching 40% for training accuracy, but still got 0% in validation accuracy.\n","\n","* For the one-shot learning on neural codes, we run several experiments extracting\n"," the encoding in different hidden layers and on average we obtained an accuracy of maximum\n","10%. The neural codes are obtained from hidden layer L and then computed the distance \n","between the test image and all the images of the support set. We consider that the shot is\n","correct if the index where the minimum distance was found is the same as the index where the target\n","had 1."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p-gkaM1tCThc"},"source":["***\n","## Question 2: Triplet networks & one-shot learning (10pt)\n","\n","### Task 2.1: Train a triplet network\n","**a)**\n","* Train a triplet network on the first 80 classes of (the training set of) Cifar-100.\n"," \n","* Make sure the network achieves a smaller loss than the margin and the network does not collapse all representations to zero vectors. *HINT: If you experience problems to achieve this goal, it might be helpful to tinker the learning rate.*\n","\n","* You are provided with a working example of triplet loss implementation for Keras below. You may directly use it.\n","\n","You may ignore the test set of Cifar-100 for this question as well. It suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n","\n","```python\n","# Notice that ground truth variable is not used for loss calculation. It is used as a function argument to by-pass some Keras functionality. This is because the network structure already implies the ground truth for the anchor image with the \"positive\" image.\n","import tensorflow as tf\n","def triplet_loss(ground_truth, network_output):\n","\n","    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n","    \n","    for embedding in [anchor, positive, negative]:\n","        embedding = tf.math.l2_normalize(embedding)\n","\n","    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n","    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n","    \n","    margin = # define your margin\n","    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n","    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n","\n","    return loss\n","```\n"]},{"cell_type":"code","metadata":{"id":"lb68ziPtSsjP","colab_type":"code","colab":{}},"source":["keras.backend.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5S7OZeg9FN_O","colab_type":"code","colab":{}},"source":["# triplet loss function\n","\n","def triplet_loss(ground_truth, network_output):\n","\n","#     anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n","\n","#     for embedding in [anchor, positive, negative]:\n","#         embedding = tf.math.l2_normalize(embedding)\n","        \n","    anchor, positive, negative = (tf.math.l2_normalize(x) for x in tf.split(network_output, num_or_size_splits=3, axis=1))\n","\n","    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n","    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n","\n","    margin = 0.4 # define your margin\n","    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n","    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAO6GN5iJbbU","colab_type":"code","colab":{}},"source":["adam_optim = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FIHD8l_rRLy5","outputId":"029c5a79-2958-4fe2-9d43-5dea3da282e7","executionInfo":{"status":"ok","timestamp":1558393860463,"user_tz":-120,"elapsed":764,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":728}},"source":["# create tripet model\n","input_shape = (32, 32, 3)\n","\n","anchor_input = Input(input_shape)\n","positive_input = Input(input_shape)\n","negative_input = Input(input_shape)\n","\n","\n","tNet = Sequential()\n","\n","tNet.add(Conv2D(128,(7,7), padding='same', input_shape = input_shape, activation='relu', name='conv1'))\n","tNet.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool1'))\n","\n","tNet.add(Conv2D(256,(5,5),padding='same',activation='relu',name='conv2'))\n","tNet.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool2'))\n","\n","tNet.add(Flatten(name='flatten'))\n","tNet.add(Dense(4,name='embeddings'))\n","\n","tNet.summary()\n","\n","\n","# encode each of the two inputs into a vector with the convnet\n","encoded_anchor = tNet(anchor_input)\n","encoded_positive = tNet(positive_input)\n","encoded_negative = tNet(negative_input)\n","\n","merged_encoding = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n","\n","\n","triplet_net = Sequential()\n","triplet_net = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_encoding)\n","\n","triplet_net.compile(loss=triplet_loss, optimizer=adam_optim)\n","\n","triplet_net.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1 (Conv2D)               (None, 32, 32, 128)       18944     \n","_________________________________________________________________\n","pool1 (MaxPooling2D)         (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv2 (Conv2D)               (None, 16, 16, 256)       819456    \n","_________________________________________________________________\n","pool2 (MaxPooling2D)         (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 16384)             0         \n","_________________________________________________________________\n","embeddings (Dense)           (None, 4)                 65540     \n","=================================================================\n","Total params: 903,940\n","Trainable params: 903,940\n","Non-trainable params: 0\n","_________________________________________________________________\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 32, 32, 3)    0                                            \n","__________________________________________________________________________________________________\n","sequential_3 (Sequential)       (None, 4)            903940      input_3[0][0]                    \n","                                                                 input_4[0][0]                    \n","                                                                 input_5[0][0]                    \n","__________________________________________________________________________________________________\n","merged_layer (Concatenate)      (None, 12)           0           sequential_3[1][0]               \n","                                                                 sequential_3[2][0]               \n","                                                                 sequential_3[3][0]               \n","==================================================================================================\n","Total params: 903,940\n","Trainable params: 903,940\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aH-wCLZ4Khy-","colab_type":"code","colab":{}},"source":["\n","# Generate the triplets\n","def Generate_Triplets(X, Y, k = 20):\n","\n","  triplet_pairs = []\n","  pairs = tuple([X, Y])\n","  y_sorted = np.unique(Y)\n","\n","  for category in y_sorted:\n","      same_class_indx = np.where(pairs[1] == category)[0]\n","      diff_class_indx = np.where(pairs[1] != category)[0]\n","\n","      A_P_pairs = random.sample(list(permutations(same_class_indx, 2)), k = k) #Generating Anchor-Positive pairs\n","      Neg_idx = random.sample(list(diff_class_indx), k = k)\n","\n","\n","      for ap in A_P_pairs[:int(len(A_P_pairs))]:\n","          Anchor = X[0][ap[0]]\n","          Positive = X[0][ap[1]]\n","          for n in Neg_idx:\n","              Negative = X[0][n]\n","              triplet_pairs.append([Anchor,Positive,Negative]) \n","\n","  return np.array(triplet_pairs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xaysqJxDBYhy","colab_type":"code","outputId":"f62cd910-405b-469e-d3db-c3addfbedc4f","executionInfo":{"status":"ok","timestamp":1558394088750,"user_tz":-120,"elapsed":195456,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":515}},"source":["# triplet Train and Test\n","triplet_train_pairs = Generate_Triplets(x_train, y_train, k=20)\n","triplet_test_pairs = Generate_Triplets(x_test, y_test, k=20)\n","\n","Anchor = triplet_train_pairs[:,0,:].reshape(-1,32,32,3)\n","Positive = triplet_train_pairs[:,1,:].reshape(-1,32,32,3)\n","Negative = triplet_train_pairs[:,2,:].reshape(-1,32,32,3)\n","\n","Anchor_test = triplet_test_pairs[:,0,:].reshape(-1,32,32,3)\n","Positive_test = triplet_test_pairs[:,1,:].reshape(-1,32,32,3)\n","Negative_test = triplet_test_pairs[:,2,:].reshape(-1,32,32,3)\n","\n","Y_dummy = np.empty((Anchor.shape[0],300))\n","Y_dummy2 = np.empty((Anchor_test.shape[0],1))\n","\n","triplet_net.fit([Anchor,Positive,Negative],y=Y_dummy, validation_split=0.2, batch_size=100, epochs=30, callbacks=[EarlyStopping(monitor='loss', patience=1)])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25600 samples, validate on 6400 samples\n","Epoch 1/30\n","25600/25600 [==============================] - 15s 591us/step - loss: 0.3799 - val_loss: 0.3793\n","Epoch 2/30\n","25600/25600 [==============================] - 15s 583us/step - loss: 0.3783 - val_loss: 0.3794\n","Epoch 3/30\n","25600/25600 [==============================] - 15s 586us/step - loss: 0.3765 - val_loss: 0.3791\n","Epoch 4/30\n","25600/25600 [==============================] - 15s 572us/step - loss: 0.3754 - val_loss: 0.3782\n","Epoch 5/30\n","25600/25600 [==============================] - 15s 568us/step - loss: 0.3746 - val_loss: 0.3792\n","Epoch 6/30\n","25600/25600 [==============================] - 15s 566us/step - loss: 0.3740 - val_loss: 0.3787\n","Epoch 7/30\n","25600/25600 [==============================] - 15s 574us/step - loss: 0.3736 - val_loss: 0.3799\n","Epoch 8/30\n","25600/25600 [==============================] - 15s 575us/step - loss: 0.3733 - val_loss: 0.3800\n","Epoch 9/30\n","25600/25600 [==============================] - 15s 577us/step - loss: 0.3732 - val_loss: 0.3802\n","Epoch 10/30\n","25600/25600 [==============================] - 15s 571us/step - loss: 0.3731 - val_loss: 0.3802\n","Epoch 11/30\n","25600/25600 [==============================] - 15s 570us/step - loss: 0.3731 - val_loss: 0.3806\n","Epoch 12/30\n","25600/25600 [==============================] - 15s 571us/step - loss: 0.3731 - val_loss: 0.3807\n","Epoch 13/30\n","25600/25600 [==============================] - 15s 571us/step - loss: 0.3731 - val_loss: 0.3809\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f48f92d7e80>"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XHGJp45AR1qm"},"source":["***\n","\n","### Task 2.2: One-shot learning with triplet neural codes\n","**a)**\n","* Use neural codes from the triplet network with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction.\n","* Explicitly state the accuracy."]},{"cell_type":"code","metadata":{"id":"bVmUXcEyKA3G","colab_type":"code","colab":{}},"source":["def make_oneshot_task_triplet(N, X, Y):\n","    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n","    \"\"\" Just the first index 0 will have the true (both images coming from the same class)!! \"\"\"\n","    n_classes, n_examples, w, h, d = X.shape\n","    \n","    # choose 21-random (1 anchor, 20 positive) indexes from the 500 examples\n","    indices_positive = np.random.choice(range(n_examples), size=(N + 1,), replace=False)\n","    \n","    # choose 20-random (20 negative) indexes from the 500 examples\n","    indices_negative = np.random.choice(range(n_examples), size=(N,), replace=False)\n","    \n","    # choose 21-random  (1 anchor/positive and 20 negative) since replace is False the index 0 will contain the True Unique category\n","    categories = np.random.choice(range(n_classes), size=(N+1,), replace=False)\n","    #print(categories)\n","    \n","    # we choose the category of the first index as the true one\n","    true_category = categories[0]\n","    \n","    # choose 2 random indexes for 2 examples comparison?\n","    #ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n","    \n","    # anchor_set.shape = (20, 32, 32, 3) This is a set of 20 images of the true category???? Is repeated 20 times the same image??\n","    anchor_set = np.asarray([X[true_category, indices_positive[0], :, :]]*N).reshape(N, w, h, d)\n","    \n","    # positive_set\n","    positive_set = X[true_category, indices_positive[1:N+1], :, :]\n","\n","    # negative_set\n","    negative_set = X[categories[1:N+1], indices_negative, :, :]\n","    \n","    anchor_set, positive_set, negative_set = shuffle(anchor_set, positive_set, negative_set)\n","    \n","    return anchor_set, positive_set, negative_set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7w6o8xIXUADN","outputId":"d88d0ba4-25de-46d3-fa2d-9ff6e38e49e4","executionInfo":{"status":"ok","timestamp":1558378434502,"user_tz":-120,"elapsed":644,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# L from 7 is Flat layer!\n","N = 19;  L = 4\n","\n","outputs = [layer.output for layer in tNet.layers]  \n","print('Amount of stacked layers: ', len(outputs))\n","\n","encoded_state = K.function(\n","[tNet.layers[0].input],  # we will feed the function with the input of the first layer\n","[tNet.layers[L].output,] # we want to get the output of the second layer\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Amount of stacked layers:  6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QWh61g-zTjOw","colab_type":"code","outputId":"08144115-1db4-42b4-a56a-0b0675e15ceb","executionInfo":{"status":"ok","timestamp":1558378617668,"user_tz":-120,"elapsed":172280,"user":{"displayName":"Jose Díaz Mendoza","photoUrl":"https://lh3.googleusercontent.com/-Nz1dE2VheTM/AAAAAAAAAAI/AAAAAAAAAE8/lcYVOvK-c5k/s64/photo.jpg","userId":"01664886430008847650"}},"colab":{"base_uri":"https://localhost:8080/","height":1159}},"source":["def test_oneshot_triplet(X, Y, N=19, k=250, verbose=True):\n","    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n","    \n","    # number of correct tasks\n","    print('Number of Layers to extract the encoding: %d \\n\\n' % L)\n","    n_correct = 0\n","    if verbose:\n","        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n","    \n","    for i in range(k):\n","        # inputs is the pair of images\n","        anchor, positive, negative = make_oneshot_task_triplet(19, x_test, y_test)\n","        \n","        # test_image we just take one since the rest of the array is duplicated => shape is (1,32,32,3)\n","        anchor_image = anchor[0].reshape(1, 32,32,3)\n","        \n","        # we initialize min_distance variable\n","        min_distance = 1000000;   max_distance = 0\n","        positive_distance_list = []\n","        negative_distance_list = []\n","        \n","        # for loop over support_set array\n","        for i in range(0, positive.shape[0]):\n","            positive_image = positive[i].reshape(1,32,32,3)\n","            negative_image = negative[i].reshape(1,32,32,3)\n","            \n","            #print(anchor_image.shape)\n","            #print(positive_image.shape)\n","            #print(negative_image.shape)\n","\n","            # extract neural code from layer L\n","            neural_code_anchor_image = np.asarray(encoded_state([anchor_image]))\n","            neural_code_positive_image = np.asarray(encoded_state([positive_image]))\n","            neural_code_negative_image = np.asarray(encoded_state([negative_image]))\n","\n","            # Compute the L2 distance\n","            positive_distance = np.linalg.norm(neural_code_anchor_image - neural_code_positive_image)\n","            negative_distance = np.linalg.norm(neural_code_anchor_image - neural_code_negative_image)\n","            positive_distance_list.append(positive_distance)\n","            negative_distance_list.append(negative_distance)\n","\n","            # update the min_distance\n","            if(negative_distance < min_distance):\n","                min_distance = negative_distance\n","                #print('\\nFound a new min_distance: {}. Image set index: {}'.format(min_distance, i))\n","                \n","            if(positive_distance > max_distance):\n","                max_distance = positive_distance\n","\n","\n","        if np.argmax(negative_distance_list) == np.argmin(positive_distance_list):\n","            n_correct += 1\n","            #print('CORRECT!!!!')\n","    percent_correct = (100.0*n_correct / k)\n","    if verbose:\n","        print(\"Got an average of {}% accuracy for {}-way one-shot learning during this loop\".format(percent_correct, N))\n","    return percent_correct\n","\n","\n","\n","\n","# Train and Test the architecture!\n","loops = 10\n","best_acc = 0\n","epochs = 1\n","for i in range(loops):\n","    print(\"=== Loop {} ===\".format(i+1))\n","    # train(siamese_net, x_train, epochs=epochs)\n","    test_acc = test_oneshot_triplet(x_test, y_test)\n","    if test_acc >= best_acc:\n","        print(\"New best one-shot accuracy of {}%  ...\".format(test_acc))\n","        #siamese_net.save(\"siamese_cifar100.h5\")\n","        best_acc = test_acc\n","        \n","print('\\nBest accuracy obtained during whole training {}%.'.format(best_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=== Loop 1 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 6.4% accuracy for 19-way one-shot learning during this loop\n","New best one-shot accuracy of 6.4%  ...\n","=== Loop 2 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 4.4% accuracy for 19-way one-shot learning during this loop\n","=== Loop 3 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 3.2% accuracy for 19-way one-shot learning during this loop\n","=== Loop 4 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 8.0% accuracy for 19-way one-shot learning during this loop\n","New best one-shot accuracy of 8.0%  ...\n","=== Loop 5 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 4.4% accuracy for 19-way one-shot learning during this loop\n","=== Loop 6 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 5.6% accuracy for 19-way one-shot learning during this loop\n","=== Loop 7 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 6.0% accuracy for 19-way one-shot learning during this loop\n","=== Loop 8 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 4.0% accuracy for 19-way one-shot learning during this loop\n","=== Loop 9 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 6.0% accuracy for 19-way one-shot learning during this loop\n","=== Loop 10 ===\n","Number of Layers to extract the encoding: 4 \n","\n","\n","Evaluating model on 250 random 19-way one-shot learning tasks ...\n","Got an average of 8.4% accuracy for 19-way one-shot learning during this loop\n","New best one-shot accuracy of 8.4%  ...\n","\n","Best accuracy obtained during whole training 8.4%.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CCcmbz0UU7mR"},"source":["***\n","## Question 3: Performance comparison (3pt)\n","\n","\n","**a)** What accuracy would random guessing achieve (on average) on this dataset? Motivate your answer briefly."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BKGDydqsVVX1"},"source":["The average accuracy for random guessing would be 1/20 = 0.05 which is 5%\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5KLXRv-eV04Q"},"source":["**b)** Discuss and compare the performances of networks in tasks 1.1, 1.2 and 2.2. Briefly motivate and explain which task would be expected the highest accuracy. Explain the reasons of the accuracy difference if there are any. If there is almost no difference accuracy, explain the reason for that."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"71kTHFBkcjp8"},"source":["* 1.1 Best accuracy obtained during whole training 21.2%. siamese for Cifar-100 250 random task on 20 pairs of images.\n","* 1.2 CNN test accuracy always 0, training accuracy >40%, with neural codes one shot learning 8%. \n","* 2.2 Triplet network accuracy varies in different tasks with maximum reaching 8%.\n","\n","Traditional Siamese network is expected to perform better than the other two it works on a distance based loss calculating model as opposed to classification models like CNN. It basically learns what makes a pair of input similar or dissimilar by focussing on the embeddings that place the two classes together, rather than trying to learn how to identify or classify a particular image. The way a siamese network works makes it robust to smaller training data because a few pairs of images are sufficient to recognize the same image in the future. The main difference is that siamese network recognizes and the CNN predicts, which is why siamese performs better for incoherent or small dataset."]},{"cell_type":"markdown","metadata":{"id":"jOyVGdtBauGS","colab_type":"text"},"source":["***\n","## Question 4: Peer review (0pt)\n","\n","Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"]},{"cell_type":"markdown","metadata":{"id":"9xDr946AauGU","colab_type":"text"},"source":["The assignment is hard and time consuming enough for being unable to do all of it alone. In order to understand every step properly, write code, decode the given code, without working together it is impossible. At some point we have not succeeded in getting correct results but we put a lot of effort in trying different models out."]}]}